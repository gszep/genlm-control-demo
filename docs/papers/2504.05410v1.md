Preprint. Under review.

Fast Controlled Generation from Language Models with
Adaptive Weighted Rejection Sampling

1 Benjamin LeBrun∗

Benjamin Lipkin∗
David R. MacIver8 Li Du6 Jason Eisner6 Ryan Cotterell2 Vikash Mansinghka1
Timothy J. O’Donnell‡3,4,5Alexander K. Lew‡1,7 Tim Vieira‡2
1MIT 2ETH Zürich 3McGill 4Canada CIFAR AI Chair 5Mila
6Johns Hopkins 7Yale 8CHI FRO

5 Jacob Hoover Vigly1 João Loula1

Abstract

The dominant approach to generating from language models subject to
some constraint is locally constrained decoding (LCD), incrementally sampling
tokens at each time step such that the constraint is never violated. Typically,
this is achieved through token masking: looping over the vocabulary and
excluding non-conforming tokens. There are two important problems
with this approach. (i) Evaluating the constraint on every token can be
prohibitively expensive—LM vocabularies often exceed 100, 000 tokens.
(ii) LCD can distort the global distribution over strings, sampling tokens
based only on local information, even if they lead down dead-end paths.
This work introduces a new algorithm that addresses both these problems.
First, to avoid evaluating a constraint on the full vocabulary at each step of
generation, we propose an adaptive rejection sampling algorithm that typically
requires orders of magnitude fewer constraint evaluations. Second, we
show how this algorithm can be extended to produce low-variance,
unbiased estimates of importance weights at a very small additional cost—
estimates that can be soundly used within previously proposed sequential
Monte Carlo algorithms to correct for the myopic behavior of local constraint
enforcement. Through extensive empirical evaluation in text-to-SQL, molec-
ular synthesis, goal inference, pattern matching, and JSON domains, we
show that our approach is superior to state-of-the-art baselines, supporting
a broader class of constraints and improving both runtime and performance.
Additional theoretical and empirical analyses show that our method’s
runtime efficiency is driven by its dynamic use of computation, scaling
with the divergence between the unconstrained and constrained LM, and
as a consequence, runtime improvements are greater for better models.

1

Introduction

Many tasks in scientific and engineering disciplines can be approached through controlled
generation of strings from a language model subject to hard constraints. For example,
we may seek to generate an API call that matches an endpoint, produce SQL queries that
are consistent with the schema of a database, or design a molecule that satisfies a target
specification. The dominant approach in these settings is locally constrained decoding
(LCD), which forces each sampled token to conform to the constraint (Lu et al., 2021; Shin
et al., 2021; Scholak et al., 2021; Lu et al., 2022; Poesia et al., 2022; Shin & Van Durme, 2022;
Geng et al., 2023; Beurer-Kellner et al., 2024; Huang et al., 2024; Moskal et al., 2024; Ugare
et al., 2024; Wang et al., 2024a; Zheng et al., 2024; Banerjee et al., 2025).1

This approach suffers from two critical drawbacks. First, the typical method for sampling
from this distribution by enumerative token masking can be slow. Full masking requires

∗co-first authorship, ‡co-senior authorship. contact: lipkinb@mit.edu & tim.f.vieira@gmail.com
1This paper focuses on runtime control, but we also note that many training-based control methods exist
(e.g., Ziegler et al., 2019; Stiennon et al., 2020; Bai et al., 2022; Ouyang et al., 2022; Rafailov et al., 2023).

1

Preprint. Under review.

checking the constraint against every item in the vocabulary—which can often consist of
more than 100, 000 tokens—before finally filtering, renormalizing, and sampling. In some
special cases, such as when the constraint can be expressed as a regular or context-free gram-
mar, optimizations are available that make checking every token feasible (Willard & Louf,
2023; Kuchnik et al., 2023; Koo et al., 2024; Ugare et al., 2024; Dong et al., 2024). However, for
black-box constraints, such optimizations do not apply. Second, as often observed (e.g., Lew
et al., 2023; Park et al., 2024; Ahmed et al., 2025; Loula et al., 2025), by renormalizing only the
local distribution (over tokens), this approach can distort the global distribution (over strings).
Because LCD myopically enforces constraints at each step, it can sometimes greedily sample
its way into low-probability regions of sequence space (see §2 for discussion).

Several papers have noted this problem and proposed approaches that recast controlled
generation as probabilistic conditioning, treating the problem as posterior inference
(Rosenfeld et al., 2001; Miao et al., 2020; Krause et al., 2021; Yang & Klein, 2021; Meng
et al., 2022; Qin et al., 2022; Shih et al., 2023; Zhang et al., 2023; Hu et al., 2024; Zhang et al.,
2024a). In this framework, approximate inference methods—e.g., importance sampling
(IS) or sequential Monte Carlo (SMC)—can be used to correct sampled sequences from the
locally constrained distribution to the target global posterior (Börschinger & Johnson, 2011;
Dubbin & Blunsom, 2012; Yang & Eisenstein, 2013; Buys & Blunsom, 2015; Lin & Eisner,
2018; Lew et al., 2023; Zhao et al., 2024; Puri et al., 2025; Loula et al., 2025).

C

C

p0(x)1

(x), where p0 is the LM next-token-distribution over the vocabulary

A key diagnostic quantity for assessing the quality of a generated sequence is the marginal
probability of the constraint under the LM’s local next-token distribution at each timestep,
Z def= ∑x
,
X
and 1
∈X
checks whether a given token conforms to the constraint at the current time step.
Within sequential IS or SMC, Z arises as an incremental importance weight update. When
this quantity is low, we have reached a point in generation where it is difficult to sample
any continuation of the sequence that satisfies the constraint. For SMC specifically, this
can be used as a signal for reallocating computation to more promising sequences during
subsequent resampling. Using token masking, it is clear how to compute Z—simply sum
over all unmasked tokens. However, if we would like to sample from the local constrained
token distribution without evaluating 1
over the entire vocabulary, this quantity may not
be directly available, and we must seek to estimate it.

C

In this paper, we introduce an exact sampler for constrained token distributions whose
runtime is faster than token masking, often by orders of magnitude. Our algorithm is
based on rejection sampling and, thus, is a Las Vegas algorithm, i.e., an exact algorithm
with a stochastic runtime. Rather than using fixed compute for each step, compute scales
dynamically based on need. Furthermore, we derive unbiased estimators of Z for this
algorithm that can be used to correct samples globally in SMC.

Our core contributions are as follows:

• A fast Las Vegas sampling algorithm compatible with any constraint. We develop adap-
tive weighted rejection sampling (AWRS), a sampling algorithm for constrained generation.
Like other rejection sampling algorithms, the cost of our approach scales with the diffi-
culty of the constraint. However, our adaptive algorithm outperforms standard rejection
sampling by capitalizing on the peakedness of the next token distribution. Due to the
speed of this algorithm, we can comfortably evaluate arbitrary black-box constraints.
• Stochastic estimates of Z to correct for greediness. We develop an approach to calculate
low-variance, unbiased estimates of Z for AWRS, supporting its integration within
approximate inference methods like SMC.

• Runtime analysis. We theoretically and empirically characterize the work done by
constrained decoding in terms of the probabilistic update between the LM’s unconditional
next token distribution and the conditioned target distribution. Critically, AWRS runs
faster when the base model is better able to capture the constraint—meaning that our
approach is more efficient for more accurate LMs.

• Empirical evaluations. We evaluate AWRS alongside several state-of-the-art baselines
on five challenging controlled generation benchmarks. AWRS yields improvements to
expressiveness, runtime, and accuracy across relevant comparisons.

2

Preprint. Under review.

Figure 1: Our approach (bottom) compared to enumerative token masking (top). Only a
subset of the tokens are checked while sampling from the same distribution.

2 Background

Locally Constrained Decoding (LCD). A language model P0 over an alphabet
is a
X ∗. We write x for individual tokens and x
probability distribution over the set of strings
for complete strings. A global constraint function 1
encodes the set of valid
L ⊆ X ∗. The problem of controlled generation can be expressed as sampling x
strings
P,
where P def= P0(
), i.e., sampling a complete string from the LM that satisfies the
constraint while preserving the relative probabilities of possible strings. This latter property
is what distinguishes proper global conditioning from arbitrary constrained sampling.

X ∗ → {

∈ L

0, 1

· |

∼

X

}

x

L

:

During the autoregressive generation of a string, it is sometimes possible to evaluate
whether sampling any given token would cause an immediate violation of the constraint.
For example, if a constraint requires writing a sentence where no word exceeds 5 characters,
the only possible continuations to “north” should induce word boundaries, rather than
continuing with “ern” or “east”. At each step of generation, based on a current string prefix
xp, we assume access to a local constraint function 1
, encoding a set of valid
=
next tokens
. That is, if the local constraint
, such that x
function rejects a token x, there is no valid continuation of xp that begins with x.

xs. xp x xs ̸∈ L

X → {

C ⊆ X

⇒ ∀

̸∈ C

0, 1

}

C

:

Fixing a prefix xp, let p0 denote the LM’s local prior distribution over the next token—note
this distribution is conditioned on the preceding tokens. At each step, LCD samples a next
token from the local posterior distribution p induced by the local constraint function:

p(x) def=

1
Z

p(x)

p(x) def= p0(x) 1

(x)

C

p(x)

Z def= ∑
∈X

x

(1)

(cid:101)

p is its unnormalized density and Z is its normalizing constant. (Note 0 < Z
where
1.)
Intuitively, Z is the total probability of satisfying the local constraint under p0. The most
common approach to implementing LCD is token masking: computing p exactly during
generation, by evaluating 1
on every item in the vocabulary, zeroing impossible tokens,
summing p0(x) over the remainder to compute Z, and renormalizing to obtain p (Fig. 1, top).

≤

(cid:101)

(cid:101)

(cid:101)

C

By applying constraints locally at every token, LCD can greedily sample its way into
low-probability sequences that are difficult or impossible to recover from. For example,
continuing with the constraint of generating a sentence where no word may exceed 5
characters, an LM will struggle to complete the prefix “The Fed says the cost of a 30-yr fixed
mortg”, despite there having been no issues up until that point (see Lew et al., 2023).

Interestingly, such dead-ends can be readily detected using precisely the quantity Z com-
puted as a byproduct of LCD. When Z is very low, we have reached a point in string
generation where only very few, improbable tokens can continue the string. 2 This local Z

2Why Z and not sequence probability? While methods optimizing sequence probability, like beam
search, partially address the sample quality desiderata, there are caveats. If there are many valid
tokens, whose sum is large, but each individual token has low probability, sequence probability will
unfairly penalize any token (Koehn & Knowles, 2017; Murray & Chiang, 2018; Cohen & Beck, 2019;
Stahlberg & Byrne, 2019). Methods that account for Z do not suffer this drawback.

3

Check every token..Check tokens.∼p0Enumerative Token MaskingAdaptive Weighted Rejection SamplingPreprint. Under review.

can be used in sequential Monte Carlo (SMC) to reweight partial generations (particles)
for subsequent re-sampling steps, which tend to eliminate unpromising string prefixes and
duplicate more promising ones (App. A and Alg. 1). In this setting—where LCD is used as
a proposal distribution within SMC—Z is precisely the incremental correction factor that
can be derived by simplifying the importance weights (Naesseth et al., 2019).

Since Z is computed as a byproduct of token masking, this algorithm provides its own
correction factors when used as a part of a proposal distribution in SMC. Unfortunately,
however, modern LMs often have vocabularies exceeding 100, 000 tokens; evaluating 1
C
on every token in this set—and, thus, exactly computing Z—is often prohibitively slow.
It would initially appear that exact sampling from the local posterior p and exact weight
correction with Z are impractical in these cases.

In the next sections, we develop an approach to sampling from the local posterior p that is
exact, tractable for large vocabularies, typically fast in terms of average time, and provides
correction factors to correct the greediness of LCD.

Simple rejection sampling. Simple rejection sampling is an algorithm that can provide
exact samples from p without looping through the entire token vocabulary. It works by
(x). If it does, the token
drawing a token x
is returned; otherwise, this process is repeated. The returned token is an exact sample from p.

p0, then checking whether the token satisfies 1

∼

C

Compared to the constant cost of token masking, this algorithm is a Las Vegas algorithm,
i.e., an exact algorithm with a stochastic runtime. It is easy to show that the expected number
of samples drawn by the algorithm before meeting the constraint is 1
Z ; it can furthermore
be shown that DKL (p
log Z (Levy, 2008; Freer et al., 2010), and thus that the
expected runtime is exponential in DKL (p
p0). When the constraint is relatively easy to
p0) is low), this can lead to runtimes that are much
satisfy (i.e., when Z
faster than those required by full-vocabulary token masking. However, when Z is very small
1), rejection sampling’s runtime can be even worse than that of token masking.
(Z <

∥
1 so DKL (p

p0) =

≈

−

∥

∥

|X |−

Adaptive rejection sampling (ARS). Adaptive rejection sampling (Gilks & Wild, 1992;
Mansinghka et al., 2009) is a version of rejection sampling that is never slower than token
masking and often significantly faster. In ARS, we adaptively remove each invalid token
encountered while rejection sampling, so that we never sample the same rejected token twice.
This simple modification can dramatically improve the runtime of the algorithm (Fig. F.2).
Both rejection sampling approaches evaluate 1
only on the samples they draw and, thus,
can be more efficient than token masking. However, neither provides a direct way to exactly
compute Z. A solution to this problem arises from the observation that for SMC, it suffices
to use independent unbiased estimates of each local Z, rather than the exact quantities, as
correction factors (App. A and Alg. 2; Naesseth et al., 2015).

C

3 Our Algorithms

In lieu of the exact but expensive correction factors obtained as a byproduct of token masking,
we wish to find cheap, unbiased estimates of Z that we can compute as a byproduct of
simple or adaptive rejection sampling.

Our first observation toward this goal is that the number of rejected tokens sampled before
generating a valid next token x contains signal about the magnitude of Z: when many
rejected tokens are generated, this suggests Z is small, and can serve as a sign that the
current prefix xp has landed us in a low-probability region of the global posterior P.
Indeed, in simple rejection sampling, the number of trials n0 + 1 (that is, n0 rejections
plus the final success) is an unbiased estimate of 1/Z. Unfortunately, 1/(n0 + 1) is not an
unbiased estimate of Z, and we require unbiased estimates of Z for sound use within SMC.
The bias is even worse in adaptive rejection sampling, because even when Z is small we may
sample few rejections, limiting n0’s usefulness as a reliable source of information about Z.

4

Preprint. Under review.

3.1 Warm-up: Weighted Rejection Sampling (WRS)

In the simple rejection sampling setting, we can collect more data about Z by running L
1
additional rejection loops. In addition to reducing variance, this also now yields an unbiased
estimate of Z. The total number of trials T (rejections and successes) required to reach L + 1
successes follows a negative binomial distribution with parameters Z and L + 1. Letting
n = ∑L

i=0 ni be the total number of rejections across the L + 1 loops,

≥

Z def=

T

L

−

=

1

L

(n + (L + 1))

=

1

L
n + L

(2)

−

(cid:98)

is a known, unbiased estimator for the Z parameter of the negative binomial distribution,
provided that L
1 (Forbes et al., 2011). This allows us to define the following algorithm
for jointly generating a next token x from p and an unbiased estimate
Definition 1. Given an unnormalized target
x,

p as above, weighted rejection sampling generates

QWRS as follows:

Z of Z.

≥

Z

(cid:98)

⟩ ∼

⟨
1. Run rejection sampling to obtain a valid sample x: Sample

(cid:101)

(cid:98)
rejections ri until obtaining an accepted token x

.

∈ C

2. For a budget of L
loop n1, ... , nL.

≥

1 additional loops, repeat step 1 and count the number of rejections on each

r1, ... , rn0, x

⟨

⟩ ∼

p0 through n0

3. Calculate the estimate
x,
4. Return

Z

⟨

⟩

Z def= L

n+L , where n = ∑L

i=0 ni.

(cid:98)

An implementation in NumPy can be found in Listing 1.

(cid:98)

Proposition 1. For

x,

Z

⟨

⟩ ∼

QWRS, x is distributed according to p and E[

Z] = Z.

Proposition 2. The expected runtime of QWRS scales with

(cid:98)

( L
Z ).

O

(cid:98)

Using these estimates, simple rejection sampling may be soundly integrated into SMC as a
proposal distribution, supporting the correction of LCD’s greediness. L = 1 is enough to
ensure unbiasedness, and we find it to work well in practice, but L can be increased to trade
higher runtime for reduced variance (Fig. F.1).

3.2 Adaptive Weighted Rejection Sampling (AWRS)

In the adaptive setting, the expected number of rejections is reduced, and the negative
binomial estimator can no longer be used. However, an auxiliary-variable argument based
on the framework of Lew et al. (2022) can be used to derive an alternative formula for
the adaptive case based on not just the number of rejections but also the probability mass
removed from p0 during adaptation.

Definition 2. Given an unnormalized target

p, AWRS generates

x,

Z

QAWRS as follows:

⟨

⟩ ∼

1. Sample

∈ C
beyond the first step, we do not sample from p0, but a re-normalized distribution on

as follows: draw n0 unique rejections ri until obtaining x

r1, ... , rn0, x

(cid:98)

(cid:101)

. Note that
r<i.

X \

⟨
n0
2. Calculate ψ0 = ∑
i=1 p0(ri).
3. Generate one additional trace

⟩

, by continuing to sample as above from the remain-
ing not-yet-rejected elements, through an additional n1 new unique rejections, until finding an
element x∗. Note that x∗ could be the same as x, since acceptances are replaced (unlike rejections).

s1, ... , sn1, x∗⟩

⟨

4. Calculate the estimate
x,
5. Return

Z

⟨

⟩

(cid:98)

Z def= 1

ψ0

n+1 , where n = n0 + n1.
−

An implementation in NumPy can be found in Listing 2.

(cid:98)

Proposition 3. For

x,

Z

⟨

⟩ ∼

QAWRS, x is distributed according to p and E[

Z] = Z.

Proposition 4. The expected runtime of QAWRS scales with
πx

(cid:98)
p0(x)+Z , i.e., the probability of each non-conforming token relative to Z.

def= p0(x)

(∑x /
∈C

πx), where

O

(cid:98)

5

Preprint. Under review.

As above, AWRS generates next tokens and correction factors suitable for use within SMC.
In addition, AWRS offers considerable runtime benefits. Trivially, since we cannot resample
rejections, we must succeed after at most
rejection steps—the number of invalid
tokens—no matter how small Z is. AWRS also has lower expected runtime. Intuitively, we
may think of the time it takes to sample an acceptance as follows. Each non-conforming
token may be considered a distractor if its individual mass is comparable to or higher than
Z, the sum of all conforming tokens. Rather than all non-conforming tokens contributing
equally, expected runtime is dominated by only these—typically rare—distractor tokens.
The exact value is derived and further explored in App. G.2.

|X \ C|

4 Experiments

Our experiments measure the practical impact of our methods on accuracy and runtime
for 5 tasks in different domains.3 We use task-specific metrics rather than evaluating the
sampler’s internal behavior (e.g., how accurately it estimates Z), but see Fig. 3 and App. F.
We compare AWRS to strong baselines through consideration of two versions: ARS-LCD,
which performs simple LCD using unweighted adaptive rejection sampling, and AWRS-SMC,
which uses the weighted version within SMC.

Methods. We first compare the following uncorrected methods in terms of both runtime
and downstream task accuracy. Methods in this section yield M = 1 unweighted samples:

• Base language model (Base LM). Sample sequences of tokens from the language model

(without forcing any constraints), i.e., sample x

P0.

∼

• Locally constrained decoding with token masking (TM-LCD). The standard approach to
constrained decoding. We mask the entire token vocabulary, re-normalize, and sample.4
• Locally constrained decoding with adaptive rejection sampling (ARS-LCD). A faster
implementation of LCD: rather than masking the entire vocabulary, we draw a sample
from the same LCD distribution using ARS. (We do not yet use an importance-weight
correction, so we run only the first rejection loop.)

The baselines above allow us to gauge the degree to which adaptive rejection sampling
improves runtime. Our next set of methods go beyond LCD, returning a weighted ensemble
of M strings such that the expected weight of x in the ensemble is P0(x)
• Sample-Verify. Sample M complete strings from the LM and weight them according to

(x) ∝ P(x).

1

L

·

1

(which amounts to discarding strings x /

).5

∈ L

L
• Sequential Monte Carlo with constraint as twist (Twisted SMC). Sample tokens directly
from the LM, but use 1
(x) as a twist function to filter partial sequences after they have
been extended with a token x. Note that this is a programmable twist as in Loula et al.
(2025), rather than a learned twist (Naesseth et al., 2019; Lawson et al., 2022).

C

• Sequential Monte Carlo with AWRS proposal (AWRS-SMC). Use the AWRS algorithm
as a proposal distribution for SMC. Like ARS-LCD, this method generates tokens using
an adaptive rejection sampling loop, but does calculate the correction factor.

3We note that our implementations are all written in pure Python and are relatively unoptimized.
Runtime improvements presented in this work are driven purely by algorithmic advancement. Being
a flexible plug-and-play method, we encourage talented systems practitioners to capture the many
untapped speedups. We outline one such example of partial concurrency in App. H.1.
4Due to the high cost of full token masking, we only include this baseline for one benchmark, from
which we illustrate our orders-of-magnitude speed-up.
5This baseline is a common approach for incorporating constraints into an LM generation pipeline
(Cobbe et al., 2021; Hendrycks et al., 2021; Nakano et al., 2021; Ahn et al., 2022; Shi et al., 2022; Uesato
et al., 2022; Olausson et al., 2023; Lightman et al., 2023; Ankner et al., 2024; Gandhi et al., 2024; Wang
et al., 2024b; Xin et al., 2024; Zhang et al., 2024b).

6

Preprint. Under review.

Metrics.

• Accuracy. The accuracy of a returned string is defined by the benchmark. For methods
that construct a weighted ensemble, we report the expected accuracy of a system that
returns a random string from this ensemble (with probability proportional to its weight).6
• Runtime. The average number of seconds it takes to generate the M complete strings.7

Benchmarks.

• Text-to-SQL (Spider). Task: Generate SQL queries from a natural language question
paired with its corresponding database schema. Data: Development split of the Spider
dataset (Yu et al., 2018). Metric: Execution accuracy (checking if the produced SQL query,
when executed on a test database, yields the same results as the ground-truth query). Base
LM: Llama 3.1 8B-Instruct. Constraint function: A Python parser for the SQL context-free
grammars provided by Roy et al. (2024) to enforce syntactically valid SQL.

• JSON. Task: Generate documents that conform to a specific JSON Schema. Data: The
validation splits of the Github-trivial, -easy and -medium tasks in the JSONSchemaBench
dataset (Geng et al., 2025). Metric: Whether a valid JSON document conforming to the
schema is generated. Base LM: Llama 3.1 8B-Instruct. Constraint function: Check the
output parses as JSON, and validate using the Python jsonschema library. Parsing is
done with a streaming JSON parser, which allows incremental detection of some schema
violations before the full document has been generated.

• Goal inference (Planetarium). Task: Formally define an agent’s goal within the STRIPS
subset of the PDDL planning language, using a natural-language description of the goal
alongside PDDL code that specifies the agent’s starting conditions and plan to reach it.
Data: Blocksworld tasks featuring up to 10 objects from the Planetarium benchmark (Zuo
et al., 2024). Metric: Equivalence to the ground-truth PDDL description. Base LM: Llama
3.1 8B. Constraint function: Check STRIPS syntax for goals as defined in the Planetarium
Blocksworld domain + execute a simulation using a ground-truth plan to verify if the
resulting state matches the predicted (partial) goal.

• Molecular Synthesis Task: Produce drug-like compounds using the SMILES nota-
tion (Weininger, 1988). Data: Few-shot prompts created by repeatedly selecting 20
random samples from the GDB-17 database (Ruddigkeit et al., 2012). Metric: Quantita-
tive Estimate of Drug-likeness (QED; Bickerton et al., 2012), a widely used measure of
molecular quality. Base LM: Llama 3.1 8B. Constraint function: A SMILES prefix validator
implemented via the Python partialsmiles library (O’Boyle, 2024).

• Pattern matching. Task: Generate strings that conform to expressive pattern-matching
specifications. Compared to formal regular expressions, these patterns contain explicit
features that cannot be fully captured by deterministic finite-state automata, including
unbounded center embedding and conditionals. Data: Over 400 pattern-matching spec-
ifications generated via the pipeline in App. I. Base LM: Llama 3.1 8B-Instruct. Metric:
Adherence to the specified pattern. Constraint function: An incremental pattern validator
that checks whether a complete match remains possible given a prefix (Barnett, 2014).

5 Results & Discussion

AWRS Outperforms State-of-the-Art Controlled Generation Methods. Tab. 1 shows the
accuracy and runtime of each method in each domain. We observe the following results:

• Controlled generation outperforms uncontrolled generation. With little overhead to

runtime, ARS-LCD improves accuracy over Base LM across all benchmarks.

∼

P, just as Base LM returns x

6The rationale is that the probability of returning x then approaches P(x) as M grows, so we approxi-
mately return x
P0. Note that we could plausibly improve accuracy
further by selecting the most probable string from the ensemble, or more generally, by the minimum
Bayes risk method of selecting or constructing a “consensus string” with low expected task loss
under the weighted ensemble.
7Our runtimes scale sublinearly in M because we use parallel hardware (a GPU). Specifically, the calls
to obtain the next-token distribution p0 from the LLM are batched over the M strings.

∼

7

Preprint. Under review.

Method

Base LM
ARS-LCD

Accuracy

Runtime (sec/ex)

0.530 (0.50, 0.56)
0.569 (0.54, 0.60)

0.79 (0.76, 0.82)
1.07 (1.01, 1.12)

Method

Base LM
ARS-LCD

Accuracy

Runtime (sec/ex)

0.683 (0.64, 0.72)
0.781 (0.74, 0.82)

2.37 (2.16, 2.59)
3.78 (3.40, 4.15)

Sample-Verify
Twisted SMC
AWRS-SMC

0.600 (0.58, 0.62)
0.596 (0.57, 0.62)
0.608 (0.58, 0.63)

2.76 (2.62, 2.91)
2.89 (2.73, 3.06)
5.33 (5.05, 5.61)

Sample-Verify
Twisted SMC
AWRS-SMC

0.845 (0.81, 0.88)
0.866 (0.84, 0.90)
0.903 (0.87, 0.93)

6.24 (5.74, 6.76)
6.31 (5.74, 6.90)
10.51 (9.61, 11.44)

(a) Text-to-SQL

(b) JSON

Method

Base LM
ARS-LCD

Accuracy

Runtime (sec/ex)

0.032 (0.01, 0.06)
0.18 (0.11, 0.26)

1.07 (0.97, 1.17)
0.77 (0.68, 0.86)

Method

Base LM
ARS-LCD

Accuracy

Runtime (sec/ex)

0.150 (0.10, 0.20)
0.568 (0.53, 0.60)

0.52 (0.50, 0.54)
0.58 (0.54, 0.62)

Sample-Verify
Twisted SMC
AWRS-SMC

0.205 (0.13, 0.28)
0.479 (0.39, 0.57)
0.528 (0.44, 0.62)

4.55 (4.25, 4.84)
3.20 (2.93, 3.47)
2.62 (2.42, 2.82)

Sample-Verify
Twisted SMC
AWRS-SMC

0.539 (0.50, 0.57)
0.549 (0.52, 0.57)
0.568 (0.54, 0.59)

1.96 (1.93, 1.99)
2.04 (1.99, 2.09)
1.52 (1.47, 1.57)

(c) Goal Inference

(d) Molecular Synthesis

Method

Accuracy

Runtime (sec/ex) Method

Accuracy

Runtime (sec/ex)

Base LM 0.570 (0.52, 0.62)
ARS-LCD 0.993 (0.98, 1.00)
0.978 (0.96, 0.99)
TM-LCD

0.10 (0.09, 0.11)
0.13 (0.11, 0.14)
6.91 (5.68, 8.46)

Sample-Verify
Twisted SMC
AWRS-SMC

0.781 (0.74, 0.82)
0.796 (0.76, 0.84)
0.990 (0.98, 1.00)

0.28 (0.26, 0.30)
0.20 (0.19, 0.22)
0.36 (0.33, 0.40)

(e) Pattern Matching

Table 1: Comparison of method accuracy and runtime across domains with 95% boot-
strapped confidence intervals. Runtime represents the average execution time (in seconds)
across all instances in the dataset. Sample-Verify and Twisted SMC were run with M = 10
particles. AWRS-SMC was run with M = 5 particles.

Figure 2: Runtime and accuracy by number of particles for AWRS-SMC and Twisted SMC.

• Adaptive sampling is much faster than token masking, with no loss of accuracy. On
the pattern matching domain — the only one where it was computationally feasible to
run TM-LCD — ARS-LCD matches its accuracy while being

faster.8

50

• Correcting for greediness improves accuracy. AWRS-SMC always matches or beats
ARS-LCD, significantly improving it in three domains (Goal Inference, JSON, and
Text-to-SQL). The other two domains (Molecular Synthesis and Pattern Matching) suffer
somewhat less under greediness because their local constraints 1
are exact, allowing
a prefix only if it has a valid continuation.

C

∼

×

• AWRS-SMC outperforms existing approaches that correct for greediness. With half
the number of particles, AWRS-SMC attains accuracy comparable to or higher than
Sample-Verify and Twisted SMC in all benchmarks.

8This 50
computation. The speedup factor modulo this constant is much greater.

speedup is at the level of complete sequence generation, including all time spent on LM

×

8

0.03.06.09.00.58 0.60 0.61 110220551020405.010.015.020.00.76 0.84 0.93 110251020401.02.03.00.15 0.35 0.56 12524100.51.01.52.00.36 0.48 0.59 24101250.20.40.60.71 0.87 1.00 110255102040Runtime (seconds per example)AccuracyText-to-SQLJSONGoal InferenceMolecular SynthesisPattern MatchingMethodAWRS SMCTwisted SMCPreprint. Under review.

Figure 3: The number of AWRS calls to 1

(y-axis) scales with DKL (p

C

p0) (Nats; x-axis).

∥

Next, we investigate how AWRS-SMC scales with the number of particles (Fig. 2) and LM
size (Fig. K.1), compared to existing methods that sample from the global distribution.

• AWRS-SMC is faster than existing SMC approaches. Across all domains, AWRS-SMC
achieves higher accuracy with lower runtime than Twisted SMC. That difference is
most extreme in the domains whose local constraints 1
are exact (Molecular Synthesis
and Pattern Matching). In that case, 1-particle AWRS-SMC outperforms Twisted SMC
with tens of particles. This suggests that the improvements in AWRS-SMC come from
including the constraints in the proposal instead of having to guess and check.

C

• AWRS-SMC with smaller LMs outperforms existing SMC approaches with larger LMs.
Fig. K.1 shows how AWRS-SMC using LLama 3.2 1B yields better runtime and accuracy
than Twisted SMC using LLama 3.1 8B and Llama 3.3 70B. This suggests that including
informative constraints in the proposal is a compute-efficient way to make small models
punch above their weight.

AWRS Achieves Speedups by Allocating Computation Dynamically. As shown through
our extensive theoretical and empirical runtime analyses (App. G.2 and Figs. G.1 and F.2),
AWRS scales with the difficulty of constraint conformance DKL (p
p0), taking less time to
sample a token when the constraint is expected and more time when it is not. We analyzed
the TM-LCD results of the pattern-matching benchmark, where token masking supports
the exact calculation of the ground truth DKL (p
p0) for each token sampling step. We
then ran AWRS for each of these steps, illustrating a few key results (Fig. 3).

∥

∥

1. DKL (p
∥
2. As DKL (p

p0) is small in most sampling steps; AWRS typically checks only 2 or 3 tokens.
p0) increases for the hardest cases, the runtime of AWRS scales dynamically.

An interesting consequence is that AWRS samples faster for more accurate base models.

∥

3. As DKL (p

∥

p0) grows, AWRS generally does not deteriorate. AWRS is roughly bounded
by the number of non-conforming tokens whose individual probabilities are close to
or exceed Z. This set is typically small, and it turns out empirically that more accurate
models seem to reduce the size of this set. Even when a model’s top choice is wrong, it
often still prefers constraint-conforming tokens to arbitrary non-conforming tokens.

6 Related Work

One approach to accelerating constrained decoding has been to pre-compile restricted
constraint classes to reduce runtime overhead. Engineering advances have enabled at least
partial compilation of constraints expressible as membership in regular (Deutsch et al., 2019;
Willard & Louf, 2023; Kuchnik et al., 2023) or context-free (Koo et al., 2024; Ugare et al.,
2024; Dong et al., 2024) languages as well as restricted classes of Boolean circuits (Ahmed
et al., 2025). In comparison, our approach supports arbitrary programmable constraints.

Another approach has explored limiting the number of constraint evaluations. Poesia et al.
(2022) and Ugare et al. (2025) allow the LM to proceed unrestrictedly and then backtrack
on errors. Scholak et al. (2021) and Shin & Van Durme (2022) use top-k truncation within
beam search. Loula et al. (2025), in a similar spirit to Morin & Bengio (2005), hierarchically
stratify the vocabulary and incrementalize the constraint checker to byte sequences. A

9

05101520DKL(pkp0)101102103104Callsto1C100101102103Frequency05101520DKL(pkp0)101102103104Callsto1C100101102103LogFrequency05101520DKL(pkp0)101102103104Callsto1C100101102103LogFrequency05101520DKL(pkp0)101102103104Callsto1C100101102103LogFrequency05101520DKL(pkp0)101102103104Callsto1C100101102103LogFrequency05101520DKL(pkp0)101102103104Callsto1C100101102103LogFrequency05101520DKL(pkp0)101102103104Callsto1C100101102103LogFrequency05101520DKL(pkp0)101102103104Callsto1C100101102103LogFrequency05101520DKL(pkp0)101102103104Callsto1C100101102103LogFrequency05101520DKL(pkp0)=°logZ101102103104epZ=Âx2Xep(x)p=˜p/Z100101102103Frequency05101520DKL(pkp0)101102103104Callsto1C100101102103LogFrequency05101520DKL(pkp0)=°logZ101102103104epZ=Âx2Xep(x)p=˜p/Z100101102103Frequency05101520DKL(pkp0)=°logZ101102103104epZ=Âx2Xep(x)p=˜p/Z100101102103Frequency05101520DKL(pkp0)=°logZ101102103104epZ=Âx2Xep(x)p=˜p/Z100101102103Frequency05101520DKL(pkp0)=°logZ101102103104epZ=Âx2Xep(x)p=˜p/Z100101102103Frequency05101520DKL(pkp0)101102103104Callsto1C100101102103Frequency051015202530DKL(pkp0)101102103104Callsto1C100101102103Frequency05101520DKL(pkp0)Llama1BLlama8BLlama70B101102103104Callsto1C100101102103Frequency05101520DKL(pkp0)Llama1BLlama8BLlama70B101102103104Callsto1C100101102103Frequency05101520DKL(pkp0)Llama1BLlama8BLlama70B101102103104Callsto1C100101102103Frequency05101520DKL(pkp0)101102103104Callsto1C100101102103FrequencyPreprint. Under review.

subset of the Outlines library (v.0.2.2; Willard & Louf, 2023) uses a variant of deterministic
probability-ordered search, first sorting tokens by their logits and then yielding the first
conforming token. Our work builds on these approaches within a probabilistic framework,
deriving an exact sampler equivalent to token masking (ARS-LCD) alongside importance
weights supporting the sound use of such samples in SMC (AWRS-SMC).

Several recent papers have used SMC to correct the greediness of LCD (Lew et al., 2023;
Zhao et al., 2024; Loula et al., 2025). These approaches come with significant limitations:
Zhao et al. (2024) require an expensive fine-tuning procedure for learning twists, whereas
Loula et al. (2025) require constraints to be decomposable into slow and fast components,
the latter of which must still be evaluated over large sets of tokens. In contrast, our approach
works with any constraint out of the box and evaluates it frugally, being typically more
accurate than twisted SMC for any fixed runtime.

7 Conclusion

Locally constrained decoding is both slow and greedy. In this paper, we address these
weaknesses respectively by introducing (i) an adaptive rejection sampler requiring orders
of magnitude fewer constraint evaluations, and (ii) an algorithm computing weights for
transforming this local sampler into a global one. Across many challenging controlled
generation domains, we find that our method is faster and more accurate than existing
methods, even when using fewer particles and smaller LMs. Finally, we use theoretical
and empirical analyses to show that the number of constraint evaluations our method
makes scales with the KL divergence between the unconstrained and constrained LM
distributions—as a consequence, our method is faster for more powerful LMs.

10

Preprint. Under review.

Reproducibility

All code and data will be released upon paper acceptance.

Author Contributions

First Authors

• Benjamin Lipkin (lipkinb@mit.edu): research conception, formal analysis (algorithms,
proper weighting), software development (samplers), experiment development (simula-
tion, pattern matching), visualization, writing

• Benjamin LeBrun (benjamin.lebrun@mail.mcgill.ca):

lead software engineer (in-
frastructure, interfaces, SMC), experiment development (text-to-SQL, JSON, pattern-
matching), visualization, writing

Contributors

• Jacob Hoover Vigly (jahoo@mit.edu): formal analysis (runtime complexity, parameter

estimator), technical advice (sampling algorithms), visualization, writing

• João Loula (jloula@mit.edu): software development (algorithm optimization), experi-

ment development (goal inference, molecular synthesis), visualization, writing

• David MacIver (david@chi-fro.org): experiment development (JSON)
• Li Du (leodu@cs.jhu.edu): software development (grammar parsing prototype)
• Jason Eisner (jason@cs.jhu.edu): technical advice (sequential inference), writing
• Ryan Cotterell (ryan.cotterell@inf.ethz.ch): organization management
• Vikash Mansinghka (vkm@mit.edu): organization management

Senior Authors

• Timothy J. O’Donnell (timothy.odonnell@mcgill.ca): organization management, senior

project leadership, project narrative development, writing

• Alexander K. Lew (alexander.lew@yale.edu): senior project leadership, research concep-
tion, formal analysis (proper weighting), project narrative development, project advising
and mentorship, writing

• Tim Vieira (tim.f.vieira@gmail.com): senior project leadership, research conception,
formal analysis (algorithms, sequential inference), software development (grammar
parsing), project narrative development, project advising and mentorship, writing

Acknowledgments

BLi is supported by a National Science Foundation Graduate Research Fellowship under
Grant No. 2141064. JHV is supported by a National Science Foundation SBE Postdoctoral
Research Fellowship under Grant No. SMA-2404644. This research was enabled in part by
compute resources provided by Mila (mila.quebec).

11

Preprint. Under review.

References

Kareem Ahmed, Kai-Wei Chang, and Guy Van den Broeck. Controllable generation via
In The Thirteenth International Conference on Learning

locally constrained resampling.
Representations, 2025. URL https://openreview.net/pdf?id=8g4XgC8HPF.

Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,
Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog,
Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jau-
regui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov,
Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada,
Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre
Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia,
Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as I can, not as I say:
Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. URL
https://arxiv.org/abs/2204.01691.

Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Daniel Chang, and Prithviraj
Ammanabrolu. Critique-out-loud reward models. In Pluralistic Alignment Workshop at
NeurIPS 2024, 2024. URL https://arxiv.org/pdf/2408.11791.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy
Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen,
Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli,
Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage,
Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam
Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham,
Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman,
Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom
Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI feedback. arXiv
preprint arXiv:2212.08073, 2022. URL https://arxiv.org/abs/2212.08073.

Debangshu Banerjee, Tarun Suresh, Shubham Ugare, Sasa Misailovic, and Gagandeep Singh.
CRANE: Reasoning with constrained LLM generation. arXiv preprint arXiv:2502.09061,
2025. URL https://arxiv.org/pdf/2502.09061.

Matthew Barnett. regex, 2014. URL https://github.com/mrabarnett/mrab-regex.

Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. Guiding LLMs the right way:
Fast, non-invasive constrained generation.
In International Conference on Machine
Learning, pp. 3658–3673. PMLR, 2024. URL https://proceedings.mlr.press/v235/
beurer-kellner24a.html.

G Richard Bickerton, Gaia V Paolini, Jérémy Besnard, Sorel Muresan, and Andrew L
Hopkins. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90–98, 2012.
URL https://www.nature.com/articles/nchem.1243.

Benjamin Börschinger and Mark Johnson. A particle filter algorithm for Bayesian word-
segmentation. In Proceedings of the Australasian Language Technology Association Workshop,
December 2011. URL https://aclanthology.org/U11-1004/.

Jan Buys and Phil Blunsom. A Bayesian model for generative transition-based dependency
parsing. In Proceedings of the International Conference on Dependency Linguistics, 2015. URL
https://aclanthology.org/W15-2108/.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint
arXiv:2110.14168, 2021. URL https://arxiv.org/pdf/2110.14168.

12

Preprint. Under review.

Eldan Cohen and Christopher Beck. Empirical analysis of beam search performance degra-
dation in neural sequence models. In International Conference on Machine Learning, pp. 1290–
1299. PMLR, 2019. URL https://proceedings.mlr.press/v97/cohen19a/cohen19a.pdf.

Daniel Deutsch, Shyam Upadhyay, and Dan Roth. A general-purpose algorithm for con-
strained sequential inference. In Proceedings of the Conference on Computational Natural
Language Learning, 2019. URL https://aclanthology.org/K19-1045/.

Yixin Dong, Charlie F Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, and Tianqi
Chen. XGrammar: Flexible and efficient structured generation engine for large language
models. arXiv preprint arXiv:2411.15100, 2024. URL https://arxiv.org/pdf/2411.15100.

Gregory Dubbin and Phil Blunsom. Unsupervised part of speech inference with particle
filters. In Proceedings of the NAACL HLT Workshop on Induction of Linguistic Structure,
Montréal, QC, 2012. URL https://aclanthology.org/W12-1907.pdf.

Catherine Forbes, Merran Evans, Nicholas Hastings, and Brian Peacock. Statistical distribu-

tions. John Wiley & Sons, 2011.

Cameron E Freer, Vikash K Mansinghka, and Daniel M Roy. When are probabilis-
tic programs probably computationally tractable.
In NIPS Workshop on Monte Carlo
Methods for Modern Applications, pp. 41, 2010. URL https://web.mit.edu/vkm/www/
FreerManRoy-NIPSMC-2010.pdf.

Kanishk Gandhi, Denise HJ Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma,
and Noah Goodman. Stream of Search (SoS): Learning to search in language. In First Con-
ference on Language Modeling, 2024. URL https://openreview.net/pdf?id=2cop2jmQVL.

Saibo Geng, Martin Josifoski, Maxime Peyrard, and Robert West. Grammar-constrained
decoding for structured NLP tasks without finetuning. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing, 2023. URL https://aclanthology.org/
2023.emnlp-main.674.pdf.

Saibo Geng, Hudson Cooper, Michał Moskal, Samuel Jenkins, Julian Berman, Nathan
Ranchin, Robert West, Eric Horvitz, and Harsha Nori. Generating structured outputs
from language models: Benchmark and studies, 2025. URL https://arxiv.org/abs/2501.
10868.

Walter R Gilks and Pascal Wild. Adaptive rejection sampling for Gibbs sampling. Journal of

the Royal Statistical Society: Series C (Applied Statistics), 41(2):337–348, 1992.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo,
Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring
coding challenge competence with APPS. arXiv preprint arXiv:2105.09938, 2021. URL
https://arxiv.org/pdf/2105.09938.

Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua
Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models.
In The Twelfth International Conference on Learning Representations, 2024. URL https://
openreview.net/pdf?id=Ouj6p4ca60.

Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor
Mordatch, Sergey Levine, Karol Hausman, and Brian Ichter. Grounded decoding: Guiding
text generation with grounded models for embodied agents. In Advances in Neural Informa-
tion Processing Systems, volume 36, 2024. URL https://proceedings.neurips.cc/paper_
files/paper/2023/file/bb3cfcb0284642a973dd631ec9184f2f-Paper-Conference.pdf.

Philipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. In
Proceedings of the First Workshop on Neural Machine Translation, pp. 28–39, 2017. URL
https://aclanthology.org/W17-3204.pdf.

Terry Koo, Frederick Liu, and Luheng He. Automata-based constraints for language model
In Conference on Language Modeling, 2024. URL https://openreview.net/

decoding.
forum?id=BDBdblmyzY.

13

Preprint. Under review.

Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty,
Richard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided
sequence generation. In Findings of the Association for Computational Linguistics: EMNLP
2021, pp. 4929–4952, 2021. URL https://aclanthology.org/2021.findings-emnlp.424.
pdf.

Michael Kuchnik, Virginia Smith, and George Amvrosiadis.

language models with RELM.
5,
93c7d9da61ccb2a60ac047e92787c3ef-Paper-mlsys2023.pdf.

Validating large
Proceedings of Machine Learning and Systems,
URL https://proceedings.mlsys.org/paper_files/paper/2023/file/

2023.

Dieterich Lawson, Allan Raventós, Andrew Warrington, and Scott Linderman. SI

O:
Smoothing inference with twisted objectives. Advances in Neural Information Processing
Systems, 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
fddc79681b2df2734c01444f9bc2a17e-Paper-Conference.pdf.

X

Roger Levy. Expectation-based syntactic comprehension. Cognition, 106(3):1126–1177, 2008.

URL https://doi.org/10.1016/j.cognition.2007.05.006.

Alexander K Lew, Marco Cusumano-Towner, and Vikash K Mansinghka. Recursive
Monte Carlo and variational inference with auxiliary variables.
In Uncertainty in
Artificial Intelligence. Proceedings of Machine Learning Research, 2022. URL https:
//proceedings.mlr.press/v180/lew22a/lew22a.pdf.

Alexander K Lew, Tan Zhi-Xuan, Gabriel Grand, and Vikash Mansinghka. Sequential Monte
Carlo steering of large language models using probabilistic programs. In ICML 2023
Workshop: Sampling and Optimization in Discrete Space, 2023. URL https://openreview.
net/pdf?id=Ul2K0qXxXy.

Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy
Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by
step. In The Twelfth International Conference on Learning Representations, 2023. URL https:
//openreview.net/pdf?id=v8L0pN6EOi.

Chu-Cheng Lin and Jason Eisner. Neural particle smoothing for sampling from conditional
sequence models. In Proceedings of the Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, 2018. URL https:
//aclanthology.org/N18-1085/.

João Loula, Benjamin LeBrun, Li Du, Ben Lipkin, Clemente Pasti, Gabriel Grand, Tianyu Liu,
Yahya Emara, Marjorie Freedman, Jason Eisner, Ryan Cotterell, Vikash Mansinghka, Alex
Lew, Tim Vieira, and Tim O’Donnell. Syntactic and semantic control of large language
models via sequential Monte Carlo. In The Thirteenth International Conference on Learning
Representations, 2025. URL https://openreview.net/pdf?id=xoXn62FzD0.

Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin
Choi. NeuroLogic Decoding: (un)supervised neural text generation with predicate logic
constraints.
In Proceedings of the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pp. 4288–4299, 2021.
URL https://aclanthology.org/2021.naacl-main.339.pdf.

Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ro-
nan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah Smith, and Yejin Choi.
NeuroLogic A⋆esque decoding: Constrained text generation with lookahead heuris-
tics. In Proceedings of the 2022 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pp. 780–799, 2022. URL
https://aclanthology.org/2022.naacl-main.57.pdf.

Vikash Mansinghka, Daniel Roy, Eric Jonas, and Joshua Tenenbaum. Exact and approximate
sampling by systematic stochastic search. In Artificial Intelligence and Statistics, pp. 400–407.
PMLR, 2009. URL https://proceedings.mlr.press/v5/mansinghka09a.html.

MegaIng. interegular, 2019. URL https://github.com/MegaIng/interegular.

14

Preprint. Under review.

Tao Meng, Sidi Lu, Nanyun Peng, and Kai-Wei Chang. Controllable text generation
with neurally-decomposed oracle. Advances in Neural Information Processing Systems,
35:28125–28139, 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/
file/b40d5797756800c97f3d525c2e4c8357-Paper-Conference.pdf.

Ning Miao, Yuxuan Song, Hao Zhou, and Lei Li. Do you have the right scissors? Tailoring
pre-trained language models via Monte-Carlo methods.
In Proceedings of the Annual
Meeting of the Association for Computational Linguistics, 2020. URL https://aclanthology.
org/2020.acl-main.314/.

Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language
model. In International workshop on artificial intelligence and statistics, pp. 246–252. PMLR,
2005. URL https://proceedings.mlr.press/r5/morin05a/morin05a.pdf.

Michal Moskal, Madan Musuvathi, and Emre Kıcıman. AI Controller Interface. https:

//github.com/microsoft/aici/, 2024.

Kenton Murray and David Chiang. Correcting length bias in neural machine translation.
In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 212–223,
2018. URL https://aclanthology.org/W18-6322.pdf.

Christian Naesseth, Fredrik Lindsten, and Thomas Schon. Nested sequential monte carlo
methods.
In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp.
1292–1301, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/
v37/naesseth15.html.

Christian A. Naesseth, Fredrik Lindsten, and Thomas B. Schön. Elements of sequential
monte carlo. Found. Trends Mach. Learn., 12(3), 2019. doi: 10.1561/2200000074. URL
https://doi.org/10.1561/2200000074.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-
pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe,
Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and
John Schulman. WebGPT: Browser-assisted question-answering with human feedback.
arXiv preprint arXiv:2112.09332, 2021. URL https://arxiv.org/pdf/2112.09332.

Noel O’Boyle. partialsmiles: A validating SMILES parser, with support for incomplete

SMILES, 2024. URL https://github.com/baoilleach/partialsmiles.

Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua
Tenenbaum, and Roger Levy. LINC: A neurosymbolic approach for logical reasoning by
combining language models with first-order logic provers. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing, 2023. URL https://aclanthology.
org/2023.emnlp-main.313/.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob
Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welin-
der, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to fol-
low instructions with human feedback.
In Advances in Neural Information Processing
Systems, 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
b1efde53be364a73914f58805a001731-Paper-Conference.pdf.

Kanghee Park, Jiayu Wang, Taylor Berg-Kirkpatrick, Nadia Polikarpova, and Loris D’Antoni.
Grammar-aligned decoding. Advances in Neural Information Processing Systems, 37:
24547–24568, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/
file/2bdc2267c3d7d01523e2e17ac0a754f3-Paper-Conference.pdf.

Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek,
and Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language
In International Conference on Learning Representations, 2022. URL https://
models.
openreview.net/forum?id=KmtVD97J43e.

15

Preprint. Under review.

Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, and Akash Srivastava. A
probabilistic inference approach to inference-time scaling of LLMs using particle-based
Monte Carlo methods. arXiv preprint arXiv:2502.01618, 2025. URL https://arxiv.org/
pdf/2502.01618.

Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. COLD decoding: Energy-
based constrained text generation with langevin dynamics. Advances in Neural Information
Processing Systems, 35:9538–9551, 2022. URL https://proceedings.neurips.cc/paper_
files/paper/2022/file/3e25d1aff47964c8409fd5c8dc0438d7-Paper-Conference.pdf.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Er-
mon, and Chelsea Finn. Direct preference optimization: Your language model is se-
cretly a reward model. Advances in Neural Information Processing Systems, 36:53728–
53741, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/
a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf.

Ronald Rosenfeld, Stanley Chen, and Xiaojin Zhu. Whole-sentence exponential lan-
guage models: A vehicle for linguistic-statistical integration. Computer Speech & Lan-
guage, 15, 01 2001. URL https://www.sciencedirect.com/science/article/abs/pii/
S0885230800901591.

Subhro Roy, Samuel Thomson, Tongfei Chen, Richard Shin, Adam Pauls, Jason Eisner, and
Benjamin Van Durme. BenchCLAMP: A benchmark for evaluating language models on
syntactic and semantic parsing. In Advances in Neural Information Processing Systems, vol-
ume 36, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/
9c1535a02f0ce079433344e14d910597-Paper-Datasets_and_Benchmarks.pdf.

Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, and Jean-Louis Reymond. Enu-
meration of 166 billion organic small molecules in the chemical universe database
gdb-17.
Journal of chemical information and modeling, 52(11):2864–2875, 2012. URL
https://pubs.acs.org/doi/pdf/10.1021/ci300415d.

Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. PICARD: Parsing incre-
mentally for constrained auto-regressive decoding from language models. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing, 2021. URL
https://aclanthology.org/2022.emnlp-main.39/.

Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I Wang. Natural
language to code translation with execution. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, 2022. URL https://aclanthology.org/2022.
emnlp-main.231/.

Andy Shih, Dorsa Sadigh, and Stefano Ermon. Long horizon temperature scaling.

In
International Conference on Machine Learning, pp. 31422–31434. PMLR, 2023. URL https:
//proceedings.mlr.press/v202/shih23a/shih23a.pdf.

Richard Shin and Benjamin Van Durme. Few-shot semantic parsing with language models
trained on code.
In Proceedings of the Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, 2022. URL https:
//aclanthology.org/2022.naacl-main.396/.

Richard Shin, Christopher Lin, Sam Thomson, Charles Chen Jr, Subhro Roy, Emmanouil An-
tonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. Con-
strained language models yield few-shot semantic parsers. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing, 2021. URL https://aclanthology.
org/2021.emnlp-main.608/.

Felix Stahlberg and Bill Byrne. On nmt search errors and model errors: Cat got your tongue?
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
pp. 3356–3362, 2019. URL https://aclanthology.org/D19-1331.pdf.

16

Preprint. Under review.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss,
Learning to summarize
Alec Radford, Dario Amodei, and Paul F Christiano.
with human feedback.
In Advances in Neural Information Processing Systems, vol-
ume 33, 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
1f89885d556929e98d3ef9b86448f951-Paper.pdf.

Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang,
Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems
with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. URL
https://arxiv.org/pdf/2211.14275.

Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, and Gagandeep Singh.
SynCode: Improving LLM code generation with grammar augmentation. arXiv preprint
arXiv:2403.01632, 2024. URL https://arxiv.org/pdf/2403.01632.

Shubham Ugare, Rohan Gumaste, Tarun Suresh, Gagandeep Singh, and Sasa Misailovic.
IterGen: Iterative structured LLM generation. In The Thirteenth International Conference on
Learning Representations, 2025. URL https://openreview.net/pdf?id=ac93gRzxxV.

Tim Vieira.

Gumbel-max

2014.
gumbel-max-trick-and-weighted-reservoir-sampling/.

URL

trick

sampling,
http://timvieira.github.io/blog/post/2014/08/01/

and weighted

reservoir

Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A Saurous, and Yoon Kim. Grammar
prompting for domain-specific language generation with large language models.
In
Advances in Neural Information Processing Systems, 2024a. URL https://openreview.net/
forum?id=B4tkwuzeiY&noteId=BaPOkLl42Y.

Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and
Zhifang Sui. Math-Shepherd: Verify and reinforce LLMs step-by-step without human
annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 9426–9439, 2024b. URL https://aclanthology.
org/2024.acl-long.510.pdf.

David Weininger. Smiles, a chemical language and information system. 1. introduction to
methodology and encoding rules. Journal of chemical information and computer sciences, 28
(1):31–36, 1988.

Brandon T Willard and Rémi Louf. Efficient guided generation for large language models.

arXiv preprint arXiv:2307.09702, 2023. URL https://arxiv.org/pdf/2307.09702.

Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan,
Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in LLMs
through large-scale synthetic data. arXiv preprint arXiv:2405.14333, 2024. URL https:
//arxiv.org/pdf/2405.14333.

Kevin Yang and Dan Klein. FUDGE: Controlled text generation with future discrim-
inators.
In Proceedings of the Conference of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Language Technologies, 2021. URL https:
//aclanthology.org/2021.naacl-main.276/.

Yi Yang and Jacob Eisenstein. A log-linear model for unsupervised text normalization. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2013. URL
https://aclanthology.org/D13-1007/.

Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma,
Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A
large-scale human-labeled dataset for complex and cross-domain semantic parsing and
text-to-SQL task. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, 2018. URL https://aclanthology.org/D18-1425.

17

Preprint. Under review.

Honghua Zhang, Meihua Dang, Nanyun Peng, and Guy Van den Broeck. Tractable control
for autoregressive language generation. In International Conference on Machine Learning.
Proceedings of Machine Learning Research, 2023. URL https://proceedings.mlr.press/
v202/zhang23g/zhang23g.pdf.

Honghua Zhang, Po-Nien Kung, Masahiro Yoshida, Guy Van den Broeck, and Nanyun Peng.
Adaptable logical control for large language models. Advances in Neural Information Pro-
cessing Systems, 37:115563–115587, 2024a. URL https://proceedings.neurips.cc/paper_
files/paper/2024/file/d15c16cf5619a2b1606da5fc88e3f1a9-Paper-Conference.pdf.

Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh
Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The 4th
Workshop on Mathematical Reasoning and AI at NeurIPS’24, 2024b. URL https://openreview.
net/pdf?id=CxHRoTLmPX.

Stephen Zhao, Rob Brekelmans, Alireza Makhzani, and Roger Baker Grosse. Proba-
bilistic inference in language models via twisted sequential Monte Carlo.
In Inter-
national Conference on Machine Learning, pp. 60704–60748. PMLR, 2024. URL https:
//proceedings.mlr.press/v235/zhao24c.html.

Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu,
Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying
Sheng. SGLang: Efficient execution of structured language model programs. In Advances
in Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=
VqkAKQibpq.

Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,
Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human prefer-
ences. arXiv preprint arXiv:1909.08593, 2019. URL https://arxiv.org/pdf/1909.08593.

Max Zuo, Francisco Piedrahita Velez, Xiaochen Li, Michael L Littman, and Stephen H Bach.
Planetarium: A rigorous benchmark for translating text to structured planning languages.
arXiv preprint arXiv:2407.03321, 2024. URL https://arxiv.org/pdf/2407.03321.

18

Preprint. Under review.

A Conditional Language Modeling

Strings. An alphabet
X
formed from the symbols in
∈ X ∗. We also define (cid:50) /
x
Constraints. Let 1
L
L ⊆ X ∗. Let ⃗
x
L

def=

|

:

X ∗ → {
x x′ ∈ L}

X
∈ X

{
the incremental constraint function:

is a finite set of symbols. Let

. Let ε denote the empty string. Let
as a special end-of-string marker.

|

|

X ∗ denote the set of all strings
denote the length of

x

0, 1
, i.e., the set of all valid prefixes of strings in

be a constraint function, encoding a set of valid strings
. We define

}

L

⃗1
L

(x′

|

x)

1
1

(cid:40)

x

∈L
x x′∈

⃗
L

if x′ = (cid:50)
otherwise

(3)

Language models. A language model P0 is a probability distribution over
probability ⃗P0(x) is the probability that a string drawn from P0 has x as a prefix:

X ∗. The prefix

P0(x x′)

⃗P0(x) def= ∑
x′∈X ∗
The conditional prefix probability is the probability that a string drawn from P0 has the
prefix x x′ given that it already has the prefix x for any strings x, x′ ∈ X ∗:
⃗P0(x x′)
x) def= P0(x)/⃗P0(x)
⃗P0(x)

and ⃗P0((cid:50)

⃗P0(x′

x) def=

|

|

(4)

(5)

Then, the probability of x may be factorized as

P0(x) = ⃗P0((cid:50)

x
|
|
∏
t=1

x)

|

⃗P0(xt |

x<t)

(6)

Global conditioning. We build on recent work defining constrained generation as proba-
bilistic conditioning (e.g., Börschinger & Johnson, 2011; Dubbin & Blunsom, 2012; Yang &
Eisenstein, 2013; Buys & Blunsom, 2015; Lin & Eisner, 2018; Miao et al., 2020; Krause et al.,
2021; Yang & Klein, 2021; Meng et al., 2022; Qin et al., 2022; Zhang et al., 2023; Hu et al.,
2024; Lew et al., 2023; Zhao et al., 2024; Park et al., 2024; Ahmed et al., 2025; Loula et al.,
2025). In particular, we define the posterior distribution P(x) of a language model prior
x

P0 subject to the condition 1

(x), as

∼

L

P(x) def=

1
G

P(x)

P(x) def= P0(x)1

(x)

L

G def= ∑
∈X ∗

x

P(x)

(7)

(cid:101)
(cid:101)
(x)] > 0. In
For P to be a well-defined probability distribution, we require G = Prx
other words, there must be a way to generate a constraint-conforming string from P0, and G
tells us the probability of this event. Note that

P0 [1

(cid:101)

∼

L

P(x) = Pr
P0
∼

X

[X = x

X

|

]

∈ L

(8)

Probabilistic conditioning is an enticing method for conditional generation as it is the only
method that preserves the relative probabilities of all events that satisfy the condition.
Note that P is a language model, so it has conditional prefix probabilities ⃗P. Unfortunately,
however, G and the conditional prefix probabilities are generally intractable to compute
exactly as they involve intractable sums over all x
∈ X ∗ (Rosenfeld et al., 2001). This makes
ancestral sampling, i.e., left-to-right sampling from the conditional prefix probability distri-
bution, infeasible. In what follows, we will develop methods for (approximate) sampling
from P.

19

Preprint. Under review.

(Sequence-level) Rejection sampling. The most straightforward algorithm for sampling
from P is rejection sampling. Note that this is the intention of sample-verify methods:

1 def rejection_sampling():
2

while True:
x
∼
if 1

L
return x

3

4

5

P0 # sample complete string from the prior

(x): # only return the string if the condition is satisfied

Rejection sampling has an expected runtime of
1.
this setting when G

≈

O

(1/G) per sample. So it is only practical in

Locally constrained decoding. Locally constrained decoding (typically performed through
token masking) defines a language model ℓ that attempts to approximate P by approximat-
ing ⃗P with⃗ℓ(x′ |
⃗ℓ(x′

⃗P(x′ |
⃗P0(x′ |

x) def=

(x′

(9)

x)

x)

x)

≈

x) where
x) ⃗1
(x′ |
L
L(x)

and L(x) def= ∑
x′∈X ∪{

⃗P0(x′
}

(cid:50)

x) ⃗1
L

|

|

|

Prior work (e.g., Lew et al., 2023; Park et al., 2024; Ahmed et al., 2025) has shown that this
local approximation can be very different (i.e., biased) from P in both theory and practice.
Fortunately, it is possible to improve this approximation (i.e., overcome this bias) with
additional computation using the techniques that we describe next.

Explanation for why locally constrained decoding is biased. Although ℓ is an effective
(x), it has a tendency to over-represent
method for generating strings x
certain strings. We can quantify this through the string’s relative probability

ℓ that satisfy 1

∼

L

because

ρ(x) def=

P(x)
ℓ(x)

=

1
G

P0(x)1
L
ℓ(x)

(x)

=

1
G

|

+1
x
|
∏
t=1

L(x<t)

P(x) =

P0(x)1
G

L

(x)

and ℓ(x) =

P0(x)1
(x)
L
+1
x
t=1 L(x<t)
∏|
|

(10)

(11)

In other words, if we compare samples from the local distribution ℓ to their relative proba-
bility in the global distribution P, the string x will appear at a rate of ρ(x) more if ρ(x) < 1
or less if ρ(x) > 1 than it should.

In this work, L(x)

1 for all x.

≤

In the expression for ρ(x), we see the factor that actually depends on the string x independent
of G, which is fortunate for us as we cannot efficiently compute G. Let w(x) = Gρ(x) =

∏|

+1

x
t=1 L(x<t).
|
+1

x
t=1 L(x<t) is an important diagnostic for sample quality.
|

Clearly, ∏|
Operationally, when sampling from ℓ, the cumulative product of local L’s is a useful signal
for detecting low-quality samples.

Interestingly, the average weight is equal to G, i.e., Ex

ℓ

∼

∏|

+1

x
t=1 L(x<t)
|

= G.

(cid:104)

(cid:105)

Although the expression for the bias does not care about the ordering, it’s hard not to think
about the operational view of ancestral sampling from ℓ; in this view, the sample is created
from left to right by sampling from the conditional token distributions. We say that our
specific sample is distorted by the product of its local normalization constants. This quantity
measures how much the constraints affected the sample. If the constraints only rule out
1.
very low (or even zero) probability tokens, then the distortion will be small as L will be

≈

This operational view motivates SMC. In the case of SMC, we compare samples of complete
t, and resample those that appear to be on track, i.e.,
and incomplete strings of length

≤

20

Preprint. Under review.

favoring those that have higher intermediate weight values. Thus, if a particle was favored
initially by the proposal distribution, it may be evicted later in favor of a replica of a
higher-weight particle.

Below is a simple example illustrating global conditioning, which can be used to illustrate
the difference with respect to local conditioning.

Example 1. Suppose that
, and the
probability distribution is encoded in the following probability tree, which be annotated with the valid
and invalid strings:

, the language of valid strings is

X

L

{

}

}

{

aa, ba

a, b

=

=

ε

a/0.9

b/0.1

a

b

a/0.01

a a ✓

b/0.99

a b ✗

a/0.99

b a ✓

b/0.01

b b ✗

Under the original distribution, we have

p(a a) = .9

p(a b) = .9

p(b a) = .1

p(b b) = .1

·

·

·

·

.01 = 0.009

.99 = 0.891

.99 = 0.099

.01 = 0.001

The globally conditioned distribution is
⃗P(a a) = .083333
⃗P(b a) = .916667
This example illustrates a local reversal: under the original distribution P0, the first symbol is a in .9
of cases, and b is the first symbol in .1 cases. However, after conditioning, the case that made a so
common (i.e., a b) is disallowed. And, less importantly, a minor case for b is disallowed. This makes
the globally conditioned conditional prefix probability:
⃗P(a

ε) = 0.009

0.009+.099 = .083333

|

⃗P(b

|

ε) = 0.099

0.009+.099 = .916667

which is a dramatic reversal from .9 and .1, respectively.

Importance sampling.
In our setting, importance sampling is a simple sampling-based
approximate inference technique that can be used to extend locally constrained data with
a weight correction. The weight correction allows us to overcome the bias in⃗ℓ
⃗P with
additional computation, i.e., taking more samples. Given a computational budget of M > 0
samples, the importance sampling procedure works as follows:

≈

1. Sample from the locally constrained distribution: x(1), ... , x(M) i.i.d.
∼

distribution such as q = ℓ.

q where q is a proposal

2. Compute weights for each m: w(m) = P0(x(m)
q(x(m))

simplifies to w(m) = ∏|

x(m)
t=1

+1

|

L(x<t).

. For the special case of q = ℓ, the weight

21

Preprint. Under review.

3. Define estimates

G def=

1
M

M
∑
m=1

w(m)

P(x) def=

1
M

M
∑
m=1

w(m)1

x=x(m)

P(x) def=

4. Return a sample from the posterior estimate

(cid:98)

(cid:98)(cid:101)

P

(cid:98)

(12)

P(x)
G
(cid:98)(cid:101)

(cid:98)

Example 2 (Example 1, continued). Returning to the above example, in this case, importance
sampling with the locally conditioned distribution as proposal generates
ℓ(a a) = .9 with weight L(a)
ℓ(b a) = .1 with weight L(b)

L(b a) = 1

L(a a) = 1

.99 = .99

.01 = .01

(cid:98)

·

·

·

·

Thus, the importance-weighted distribution estimate is

·

·

.01

.99 = .083333

P(a a) = .01

.9
·
.9+.1
P(b a) = .9+.1
(cid:98)
.9+.1
which is precisely the global distribution.
(cid:98)

.99 = .916667

.01

·

·

Sequential Monte Carlo. Sequential Monte Carlo (SMC) is an extension of importance
sampling, which effectively applies importance sampling to a sequence of intermediate
target distributions chosen to keep partial generations on track rather than once for the
entire sequence.
We define an intermediate shaping function ⃗ψ :
0, which is the key to providing
partially generated strings intermediate feedback. Complete strings, on the other hand, will
(x), but we will discuss alternatives
be judged by
shortly. In our incarnation of the SMC algorithm, both complete and incomplete strings will
evolve through a time-indexed state space where, at time t, the state contains two variables:
a string of length
t, a Boolean indicating if the string is active (incomplete). When the
string is active, its length is restricted to equal t. Once the string has been completed, it is
never changed, i.e., no symbols can be appended to it.

P. In this work, we use ⃗ψ(x) = ⃗P0(x) ⃗1
L

X ∗ →

≤

R

(cid:101)

≥

We define the initial target as

π0(α, x) def= 1

α=

,x=ε

⊤

(13a)

which says that initially, the only possible state is
string. We define the intermediate targets πt (for t > 0) as9

⟨⊤

, ε

⟩

, i.e., active and equal to the empty

πt(α, x) = ⃗ψ(x)1
P(x)1
+

x

|

x

=t,α=

|
<t,α=

⊤

|

|
) converges to

⊥

[incomplete]

(13b)

[complete; length < t]

∞, πt(

⊤

P.

→

,
(cid:101)
·

Note that as t
In this work,10 we use ⃗ψ(x) = ⃗P0(x) ⃗1
(x) as our shaping function as in (Loula et al., 2025).
L
Under this choice of intermediate target, the constraint checker ensures that we always
have at least one valid complete of the string prefix x. This feedback is useful for detecting
rejection as soon as the string is guaranteed to fail, and it is useful for detecting cases where
a lot of the probability mass from the prior has been eliminated (i.e., we are likely to have
a low-weight particle in the sense of importance sampling). In relation to ⃗P, it provides a

(cid:101)

+ 1 steps to be generated due to the final (cid:50) event. This is why πt has

9A complete string x requires
complete strings of length < t, rather than (say)
10A guiding principle for designing ⃗ψ is to approximate ⃗ψ

≤

t.

x

|

|

⃗P. However, any choice of ⃗ψ that satisfies
⃗P(x) = 0 will converge (albeit with different rates). Note
the technical condition ⃗ψ(x) = 0 =
that if ⃗ψ = ⃗P, the SMC algorithm is an exact sampler for P. Unfortunately, computing ⃗P exactly is
intractable, so we must approximate it.

⇒

≈

22

Preprint. Under review.

sufficient condition for when ⃗P(x) = 0. It, however, does not provide much information
beyond that.11

We define the shorthand

⃗ψ(x′

|

x) def=

P(x)
⃗ψ(x)
⃗ψ(x x′)
(cid:101)
⃗ψ(x)




if x′ = (cid:50)

otherwise

Importantly, this definition gives the following factorization of



P,

x

∀

∈ X ∗,

P(x) = ⃗ψ(ε)⃗ψ((cid:50)

x
|
|
∏
t=1

x)

|

⃗ψ(xt |

(cid:101)

x<t)

(14)

(15)

It is straightforward to verify that the shaped weights of complete strings are equivalent to
those used in importance sampling:

(cid:101)

⃗ψ(xt |
P(x)
⃗q(xt |
q(x)
(cid:101)
We provide pseudocode for the SMC procedure in Alg. 1.

⃗ψ((cid:50)
⃗q((cid:50)

= ⃗ψ(ε)

x)
x)

x
|
|
∏
t=1

|
|

Algorithm 1 Sequential Monte Carlo

1. procedure SMC(q, ⃗ψ, M, τ)
2.

for m = 1 ... M :

(x(m), w(m), α(m))

(ε, ⃗ψ(ε), true)

3.
4. while
5.

m

1 ... M : α(m) :
for m = 1 ... M s.t. α(m) :

∈

←

∃
q(
x′ ∼
· |
if x′ = (cid:50) :
α(m)

←

else

x(m))

false

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

20.

21.

22.

23.

24.

25.

), w(

·

·

), α(

·

), τ)

(x(

·

x(m)

x(m)
x′
◦
←
w(m) ⃗ψ(x′|
x(m))
w(m)
x(m))
q(x′|
←
), w(
RESAMPLE(x(
), α(
))
·
·
←
m=1 w(m)
1
M ∑M
1
M ∑M
P(x)
G
(cid:98)(cid:101)
P,
G,
(cid:98)

m=1 w(m)1

x = x(m)

P)

←

{

}

G

←
P(x)
(cid:98)
P(x)
(cid:98)(cid:101)
←
return (
(cid:98)

17. procedure RESAMPLE(x(
(cid:98)(cid:101)
(cid:98)
m=1 w(m)
∑M
18. W
∑M
W2/
m=1

←

M

(cid:98)

19.

w(m)

2

), w(

·

·

), α(

·

), τ)

if
(cid:98)

←
(cid:16)
M < τ
M :
·
); w(
)
)
x(
x(
·
for m = 1 ... M :
(cid:98)

←

·

·

(cid:17)

(cid:1)
)

·

w(

(cid:0)

←

∼

Categorical( 1

R
(x(m), w(m), α(m))
))
), w(

), α(

W ⟨
←

·

·

return (x(

·

w(1), ... , w(M)
(x(R), W/M, α(R))

)

⟩

x<t)
x<t)

(16)

▷M number of samples

▷Incomplete particles

▷Complete particle

▷Effective sample size
▷Resample if needed
▷Temporary copy

Extension for properly weighted token proposals. Our method uses an extension of Alg. 1 that
allows for a properly weighted proposal distribution. More specifically, the token proposals

11Note that it is ok from the perspective of the technical conditions for ⃗1
L

(i.e., strings that are considered ok, that are not); however, false negatives are prohibited.

(x) to have false positives

23

Preprint. Under review.

Algorithm 2 Sequential Monte Carlo with Properly Weighted Proposal

1. procedure SMC-PWP(q, ⃗ψ, M, τ)
2.

for m = 1 ... M :

(x(m), w(m), α(m))

(ε, ⃗ψ(ε), true)

3.
4. while
5.

←

∃

m

1 ... M : α(m) :
for m = 1 ... M s.t. α(m) :
x(m))

∈

q(

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

(x′, w′)
∼
if x′ = (cid:50) :
α(m)

←

else

· |

false

G

(x(

x(m)

x(m)
←
w(m)
w(m)
←
·
), w(
))
), α(
·
·
·
←
m=1 w(m)
1
M ∑M
1
M ∑M
P(x)
G
(cid:98)(cid:101)
P,
G,
(cid:98)

P)

←

←
P(x)
(cid:98)
P(x)
(cid:98)(cid:101)
←
return (
(cid:98)

(cid:98)

(cid:98)(cid:101)

(cid:98)

x′

◦
w′
RESAMPLE(x(

), w(

·

·

), α(

·

), τ)

m=1 w(m)1

x = x(m)

{

}

▷M number of samples

▷Incomplete particles

▷Complete particle

x(m)). The requirement is that
will now be a pair of a token and a weight, i.e., (x′, w′)
token proposal distribution is properly weighted with respect to the intermediate target:
x(m)). We provide several methods for defining these kinds of proposal distributions

q(

· |

∼

(§3). Pseudocode for the extended method is provided in Alg. 2.

⃗ψ(x′ |

24

Preprint. Under review.

B Recursive Auxiliary-Variable Inference (RAVI)

Importance sampling is an approach to approximate sampling from an unnormalized
target distribution
p, with normalizing constant Z, through a proposal distribution q whose
p. We would like to design q with favorable properties towards
support is at least that of
our downstream goal, e.g., efficient runtime. Crucially, this design of a strong proposal
distribution q is often mediated by auxiliary random choices r such that we sample
q.
When this is the case, it is necessary to calculate a weight w(x, r) to correct for r.

⟩ ∼

x, r

(cid:101)

(cid:101)

⟨

RAVI (Lew et al., 2022) gives us a generalized, flexible recipe for deriving the weights
of relatively arbitrary choices of q. Within RAVI, we may work with any unnormalized
target distribution
Q denote the process of sampling
⟨
from a proposal q and calculating its weight w. We are interested in developing Q with the
following property:
Definition 3. The proposal distribution Q is properly weighted if:

p over any space

. Let

⟩ ∼

x, w

X

(cid:101)

E
x,w

⟩∼

⟨

[w f (x)] = Z E
p
∼

x

Q

[ f (x)]

(17)

Note that by taking trivial f (x) = 1, this property implies E

Q[w] = Z.

x,w

⟨

⟩∼

B.1 The Proposal

A RAVI proposal is a joint distribution q(r, x) over a product space
holds
auxiliary random choices made by the proposal. Typically, proposals will be designed to
make the marginal q(x) a good approximation to the target p(x).

, where

R × X

R

In this work, we consider exclusively exact proposals, where q(x) = p(x). This is achieved
via various formulations of rejection sampling, where r represents the auxiliary randomness
generated by the rejection sampler. Because q(x) = ∑r
q(r, x) is typically intractable to
p(x)
evaluate exactly, the usual importance weight w =
q(x) cannot be computed directly. RAVI
provides a way to tractably compute a (noisy) importance weight that still satisfies proper
(cid:101)
weighting.

∈R

B.2 The Meta-Proposal

|

A RAVI meta-proposal h(r; x) is designed to infer r given x. The optimal meta-proposal
x) def= q(r, x)/q(x), but this optimal choice is often not tractable. In practice,
would be q(r
we select a family of probability distributions h(r; x) over
, with the
R
appropriate support, in order to define an ‘extended’ target (
p(x)h(r; x) over
q.12 For a given x, this means that h should,
the joint space. Formally, we require that
with probability 1, propose some r such that q(r, x) > 0.

, indexed by x
p h)(r, x) def=

∈ X

p h

≪

(cid:101)

(cid:101)

B.3 Properly Weighted Sampling

(cid:101)

Definition 4. We define 1-level RAVI sampling (cf. 2-level RAVI sampling in Def. 5) from a
proposal q and meta-proposal h as follows:

1. Generate (r, x)
2. Evaluate w def=
x, w
3. Return

.

q.
∼
p(x) h(r;x)
q(r,x)

.

(cid:101)

⟨

⟩

This can be seen as standard importance sampling on the extended state space
The weighted value
which means that the weighted value

.
R × X
p(x) h(r; x),
is properly weighted for the re-marginalized

is properly weighted for the extended target

⟩
12For µ, ν two distributions on the same domain, µ is said to be absolutely continuous (AC) with

x, w

r, x

, w

⟨⟨

(cid:101)

⟨

⟩

⟩

respect to ν (written µ

≪

ν) if and only if µ is zero anywhere that ν is zero.

25

Preprint. Under review.

p(x) h(r;x)

p(x). When h(r; x) performs "perfect meta-inference," i.e., when h(r; x) = q(r

x),
target
p(x)
then w =
q(x) , meaning we compute exact importance weights.
Otherwise, the importance weights will be noisier but will still have the same expected
(cid:101)
value (that is, E[w

p(x) q(r
q(r
(cid:101)
x] =

x)
x) q(x) =
|

q(r,x) =

(cid:101)

(cid:101)

|

|

p(x)
q(x) ; consequently we also have E[w] = E(r,x)
(cid:101)

∼

|

Proposition 5. Given a proposal q(r, x) and meta-proposal h(r; x), such that
properly weighted for

p.

q[

p(x)
q(x) ] = Z).
(cid:101)
p h

≪

q, Def. 4 is

(cid:101)

(cid:101)

Proof.

E
Q1-RAVI

⟩∼

x,w

⟨

[w f (x)] = E
r,x
⟩∼

f (x)

[Def. 4]

(18a)

f (x)

[Def. of expectation]

(18b)

[Cancellation; AC]

(18c)

h(r; x)

[Rearranging]

(18d)

; x) is a distribution]

(18e)

[h(

·

[Def. of p]

(18f)

(cid:21)
p(x) h(r; x)
q(r, x)

(cid:101)
p(x) h(r; x) f (x)

p(x) h(r; x)
q(r, x)

(cid:101)
q(r, x)

q (cid:20)
∑
x
∈X
∑
x
∈X
p(x) f (x) ∑
∈R

(cid:101)

r

p(x) f (x)
(cid:101)

Z p(x) f (x)
(cid:101)

[ f (x)]

⟨
= ∑
r
∈R
= ∑
r
∈R
= ∑
x
∈X
= ∑
x
∈X
= ∑
x
∈X
= Z ∑
x
∈X
= Z E
p
∼

x

p(x) f (x)

[Rearranging]

(18g)

[Def. of expectation]

(18h)

■

B.4 Deeper Recursive Inference

(so we have h(s, r; x)), then
If our meta-inference h itself introduces auxiliary variables s
we can repeat this process, adding a meta-meta-proposal j(s; r, x) that aims to approximate
h(s

r; x). Our properly weighted algorithm then becomes:

∈ S

|

Definition 5. We define 2-level RAVI sampling from a proposal q, meta-proposal h, and meta-meta-
proposal j as follows:

1. Generate (r, x)
2. Evaluate w def=
x, w
3. Return

.

; r, x).

j(

·

q and s
∼
∼
p(x) h(s,r;x)
q(r,x) j(s;r,x) .
(cid:101)

⟨

⟩

Intuitively, since h “overextended” the target distribution to include not just r but also s, we
must now extend the proposal q with a distribution j over s. In general, we can continue
this process (e.g., if j has auxiliary variables), alternately extending the model and proposal
until we reach an extension that does not introduce new auxiliary variables.

Proposition 6. Given a proposal q(r, x), meta-proposal h(s, r; x), and meta-meta-proposal j(s; r, x),
such that

q j, Def. 5 is properly weighted for

p h

p.

≪

(cid:101)

26

(cid:101)

Preprint. Under review.

Proof.

x,w

⟨

=

[w f (x)]

E
Q2-RAVI

⟩∼

E
q,s

j(

;r,x) (cid:20)

∼

·
∑
∑
x
s
∈X
∈S
∑
∑
x
s
∈X
∈S
p(x) f (x) ∑
∈R
p(x) f (x) ∑
(cid:101)
∈R

(cid:101)

r

r

p(x) f (x)
(cid:101)

Z p(x) f (x)
(cid:101)

p(x) f (x)

[ f (x)]

r,x

⟩∼

⟨
= ∑
r
∈R
= ∑
r
∈R
= ∑
x
∈X
= ∑
x
∈X
= ∑
x
∈X
= ∑
x
∈X
= Z ∑
x
∈X
= Z E
p
∼

x

p(x) h(s, r; x)
q(r, x) j(s; r, x)
(cid:101)

q(r, x) j(s; r, x)

p(x) h(s, r; x)
q(r, x) j(s; r, x)
(cid:101)
p(x) h(s, r; x) f (x)

h(s, r; x)

∑
s
∈S
h(r; x)

f (x)

(cid:21)

[Def. 5]

(19a)

f (x)

[Def. of expectation]

(19b)

[Cancellation; AC]

(19c)

[Rearranging]

(19d)

[Marginalize over s]

(19e)

; x) is a distribution]

(19f)

[h(

·

[Def. of p]

(19g)

[Rearranging]

(19h)

[Def. of expectation]

(19i)

■

B.5 RAVI Intuitions for Def. 1 (WRS)

=

In the context of the RAVI framework, WRS can be understood as representing the rejected
samples as auxiliary variables generated during the sampling process. The auxiliary space
i consists of finite lists of rejected samples. The proposal q generates a trace
R
of rejection sampling, while the meta-proposal h generates L additional rejection loops to
improve our inference of the auxiliary variables. Finally, a meta-meta-proposal j accounts
for the additional auxiliary variables introduced by h.

NX

∪i

∈

Intuitively, as L increases, we obtain a better estimate of the acceptance probability Z,
leading to lower variance in our importance weights. In the limit as L
∞, meta-proposal
→
h(r; x) converges to its optimal distribution q(r
x) and the meta-meta-proposal j(s; r, x)
r; x).
converges to its optimal distribution h(s

|

|

B.6 RAVI Intuitions for Def. 2 (AWRS)

In the context of the RAVI framework, AWRS represents a modification where the sampling
procedures in the proposal, meta-proposal, and meta-meta-proposal all maintain memory
of previously rejected samples. The sampling distributions are continually renormalized
after each rejection to account for the removed probability mass. This adaptation requires
a different weight calculation that accounts for the changing probability distributions
throughout the sampling process.

27

Preprint. Under review.

C Proofs for §3.1 (WRS)

First, we will derive the proper weights for the WRS algorithm using RAVI.
Proposition 7. QWRS is properly weighted for

p.

Proof. We will prove this using the 2-level RAVI framework from Def. 5.

(cid:101)
First, we define our spaces and distributions:

• Let the auxiliary space

=
R
obtaining an accepted sample.
and x

• For r = (r1, ... , rn)
0, 1

∈ R

):

X → {

}

i represent finite lists of samples rejected before

∪i

∈

NX

, define our proposal distribution (Remember 1

:

C

∈ X

q(r, x) = p0(xc) 1

= p0(xc) 1

(xc)

(xc)

C

C

n
∏
i=1
n
∏
i=1

p0(ri) (1

1

C

−

(ri))

p0(ri)

= p0(xc)

n
∏
i=1

p0(ri)

[Def.]

(20a)

r

[

∀

∈ R

: 1

C

(r) = 0]

(20b)

[

xc ∈ C

∀

: 1

C

(xc) = 1]

(20c)

}

= (

rn and accepting xc.
• Our meta-proposal h requires additional auxiliary variables. Let

This represents the probability of rejecting r1 ···
1, ... , L

×
, representing L additional rejection loops along with an index L∗ choosing
{
one loop. Recall h aims to infer the rejected r given x. But since x is independent of
the rejected samples that preceded it, we set h to simulate its own L rejection sampling
loop(s). To avoid this same problem at the next level of meta-inference, we leave auxiliary
randomness consisting of sequences that can be simulated by rejection sampling in j
below.

• For s = ((s(1), ... , s(L)), (s∗1, ... , s∗L), L∗)

, r
, set the meta-proposal to gener-
ate L rejection loops where each s(i) represents rejected samples, s∗i is an accepted sample,
and L∗ is the index of the split loop,13 which is chosen with probability proportional to
. Return the chosen s(L∗) truncated at the sampled split-point as
ni + 1, where ni =
the proposed latent sequence of rejected samples. Letting n0 = n for convenience:

R × X

∈ R

∈ X

∈ S

s(i)

)L

, x

S

|

|

h(s, r; x)

·

=

=

=

=

n0 + nL∗ + 1
∑L
i=0(ni + 1)
L
∏
i=1 (cid:32)

p0(s∗i ) 1

1
n0 + nL∗ + 1
ni
∏
j=1

(s∗i )

C

p0(s

(i)
j )(1

1

C

−

(s

(i)
j ))

(cid:33)

[Def.]

(21a)

1
i=0(ni + 1)

∑L

1
i=0(ni + 1)

∑L

1
i=0(ni + 1)

∑L

L
∏
i=1 (cid:32)

L
∏
i=1 (cid:32)

L
∏
i=1 (cid:32)

p0(s∗i ) 1

p0(s∗i ) 1

(s∗i )

(s∗i )

C

C

ni
∏
j=1

ni
∏
j=1

p0(s

(i)
j )(1

1

C

−

(s

(i)
j ))

(cid:33)

[Cancellation]

(21b)

p0(s

(i)
j )

(cid:33)

s(i)

[

∀

∈ S

: 1

C

(cid:16)

s(i)

= 0]

(21c)

(cid:17)

p0(s∗i )

ni
∏
j=1

p0(s

(i)
j )

(cid:33)

s∗

[

∀

∈ S

: 1

C

(s∗) = 1]

(21d)

• Our meta-meta-proposal j aims to infer the auxiliary randomness s given r. Again, r is not
useful, but the sequence of rejected samples in s can be simulated by rejection sampling.
Set j to generate L independent rejection loops and select L∗ uniformly, representing the

13Note, the terms containing L∗ will cancel out, so we do not actually need to sample it.

28

Preprint. Under review.

guess about which loop was split. Note that due to independence, we can re-use our
existing loops. j is defined as follows:

j(s; r, x) =

=

=

1
L

1
L

1
L

L
∏
i=1 (cid:32)

L
∏
i=1 (cid:32)

L
∏
i=1 (cid:32)

p0(s∗i ) 1

p0(s∗i ) 1

(s∗i )

(s∗i )

C

C

ni
∏
j=1

ni
∏
j=1

p0(s

(i)
j )(1

1

C

−

(s

(i)
j ))

(cid:33)

[Def.]

(22a)

p0(s

(i)
j )

(cid:33)

s(i)

[

∀

∈ S

: 1

C

(cid:16)

s(i)

= 0]

(22b)

(cid:17)

p0(s∗i )

ni
∏
j=1

p0(s

(i)
j )

(cid:33)

s∗

[

∀

∈ S

: 1

C

(s∗) = 1]

(22c)

Following Def. 5, and substituting our definitions, we calculate the proper weight as follows.
Note all terms of p0 and 1

will cancel out:

1

i=0(ni+1) ∏L
∑L
L ∏L

i=1 p0(ri) 1

i=1(...)

i=1(...)

w =

=

=

=

=

=

C

C

(x)

p(x)h(s, r; x)
q(r, x)j(s; r, x)
(cid:101)
p0(x)1
p0(xc) ∏n
1
i=0(ni+1)
1
L

∑L

L
i=0(ni + 1)

∑L

L
L + ∑L
i=0 ni
L
L + n

[Def. 5]

(23a)

[Substitution of defs.]

(23b)

[Cancellation; AC]

(23c)

[Algebra]

(23d)

[Algebra]

(23e)

[Def. of n]

(23f)

This matches the weight formula in Def. 1, confirming that our sampling procedure is
■
properly weighted for

p justified by the RAVI framework.

Proposition 8. If

x, w

⟨

(cid:101)
⟩ ∼

QWRS, then x is distributed according to p.

Proof.

Pr
⟩∼

QWRS

x,w

⟨

x

[x] = Pr
p0
∼
Prx

=

∼

[x

1

(x)]

|

C
p0 [x] Prx
Prx

p0 [1

(x)

∼
p0 [1

C
(x)]

C

∼
(x)

=

p0(x)1
Z

C

= p(x)

x]

|

[Def. 1]

(24a)

[Bayes rule]

(24b)

[Defs. of p0, 1

, Z]

C
[Def. of p]

(24c)

(24d)
■

C.1 Another perspective on WRS

Upon completing the RAVI derivation, it is now clear that an alternative formulation for
the weights of WRS is possible. We can consider our weight calculation as using L + 1
independent rejection-sampling loops to estimate the acceptance probability Z. The total
i=0(n + 1) required to reach b def= L + 1 acceptances in such a process
number of trials T def= ∑L
NB(b, Z). Thus, calculation of w for WRS
follows a Negative Binomial distribution, i.e., T
amounts to calculating an unbiased estimator of Z for the Negative Binomial distribution.
See §3.1 for another gloss of this perspective.

∼

29

Preprint. Under review.

Proposition 9.

Z = L

L+n is the minimum-variance unbiased estimator (MVUE) for Z.

(cid:98)

Proof. The PMF of NB(b, Z), for b

N

≥

0, Z

∈

∈

[0, 1] is:

Pr[T = t; b, Z] =

1
1

t
b

−
−

(cid:19)

(cid:18)

Zb(1

−

Z)t

−

b

supported on t

b, b + 1, b + 2, ...

∈ {

.

}

Assuming b = L + 1 is fixed, the PMF may be written in exponential family form:

Pr[T = t; b, Z] =

=

1
1
1
1

t
b
t
b

−
−
−
−

(cid:19)

(cid:19)

(cid:18)

(cid:18)

exp(b log Z + (t

b) log(1

Z))

−

−

exp

t log(1

(cid:18)

Z) + b log

−

Z

−

1

(cid:18)

Z

(cid:19)(cid:19)

(25)

(26a)

(26b)

with sufficient statistic t. Since t = b + n, b is fixed, and we consider a one-parameter
exponential family, then we can conclude that n is a complete sufficient statistic for Z.

, denoting the success of the first trial. Trivially, E[1

Let’s start with a simple unbiased estimator for Z. Consider the indicator random variable
1
] = Z. By
x0∈C
the Rao-Blackwell theorem, we can reduce the variance of this estimator by computing its
conditional expectation given the sufficient statistic n:

] = Pr[x0 ∈ C

x0∈C

Z = E[1

n]
x0∈C |
= Pr[x0 ∈ C |
Pr[n
=
|

(cid:98)

Z)n

−
Z)n

−

1)(1

((b
−
(b

n]
] Pr[x0 ∈ C
x0 ∈ C
Pr[n]
1)+n
1
1 )Z(b
−
−
1)
−
(b+n
1
1 )Zb(1
−
b
−
((b
1)+n
1
1 )Zb(1
−
−
(b
1)
−
−
(b+n
1
1 )Zb(1
−
b
−
((b
1)+n
1
1 )
−
−
(b
1)
−
−
(b+n
1
1 )
−
b
−
1
b
−
b + n

−
Z)n

−
Z)n

−

1

−
L + 1
−
L + 1 + n

1

−

1

L
L + n

=

=

=

=

=

=

]

[Def. of Expectation]

[Bayes Rule]

(27a)

(27b)

(27c)

Z

·

[Substitute Defs.]

(27d)

[Combine like terms]

(27e)

[Cancellation]

(27f)

[Algebra]

(27g)

[b=L+1]

(27h)

[Algebra]

(27i)

So,

Z = L

L+n is our estimator.

(cid:98)

To confirm that
entire support of the PMF of T′ ∼
(cid:98)

Z is unbiased, see that its expected value reduces to summing over the

NB(b

−

1, Z), scaled by constant Z.

30

Preprint. Under review.

Pr[T = t; b, Z]

[Def. of Expectation]

(28b)

(28a)

(cid:21)

−

Zb(1

Z)t

−

b

(cid:21)

Z)t

−

b

−

(cid:21)

[Substitute Defs.]

(28c)

[Binom. Coef. Identity]

(28d)

Zb(1

Z)t′−

(b

−

1)

−

(cid:21)

[Reindex s.t. t′ = t

1]

−

(28e)

1

1

(cid:19)

−

Zb

−

1(1

−

Z)t′−

(b

−

1)

(cid:21)

Pr[T′ = t′; b

1, Z]

−

[Algebra]

(28f)

[Eq. (25)]

(28g)

[Dist. Sums to 1]

(28h)

E[

Z] = E

(cid:98)

=

=

=

=

L
L + n
L
L + n

(cid:21)

b
t

−
−
t
b

(cid:20)
∞
∑
t=b (cid:20)
∞
∑
t=b (cid:20)
∞
∑
t=b (cid:20)(cid:18)
∞
∑
t′=b
1 (cid:20)(cid:18)
−
∞
∑
t′=b
−
∞
∑
t′=b
−

1

1
1

(cid:18)
2
−
2
(cid:19)
−
t′ −
b
−

(b

1 (cid:20)(cid:18)

t
b

1
1

−
−
Zb(1

(cid:19)

1
2

(cid:19)
t′ −
1)
−

= Z

= Z

= Z

(cid:98)

Therefore
we have by construction, since L

Z is an unbiased estimator of Z. Note this proof relies on b = L + 1

2, which

≥

1.

≥

By the Lehmann-Scheffé theorem, since n is a complete sufficient statistic for Z, and
an unbiased estimator that is a function of n,
estimator (MVUE) for Z.

Z is
Z must be the minimum-variance unbiased
■

(cid:98)

(cid:98)

31

Preprint. Under review.

D Proofs for §3.2 (AWRS)

Proposition 10. QAWRS is properly weighted for

p.

Proof. We will prove this using the 2-level RAVI framework from Def. 5, adapted for the
case where our sampling procedures are adaptive.

(cid:101)

First, we define our spaces and distributions with adaptivity:

• Let the auxiliary space

NX
before obtaining an accepted sample.
, we define our adaptive proposal distribution:

R
and x

• For r = (r1, ... , rn0 )

∪i

∈

i represent finite lists of distinct samples rejected

=

∈ R

∈ X

q(r, x)
n0
∏
i=1

=

(cid:32)

=

n0
∏
i=1

(cid:32)

=

n0
∏
i=1

(cid:32)

1
1
j=1 p0(r) (cid:33)
−

∑i

p0(xc) 1

(xc)

C

1
1
j=1 p0(r) (cid:33)
−

∑i

p0(xc) 1

(xc)

C

n0
∏
i=1

n0
∏
i=1

p0(ri)(1

1

C

−

(ri)) 1

[

i

=j.ri̸

∀

=rj]

[Def.]

(29a)

p0(ri) 1

[

i

=j.ri̸

∀

=rj]

r

[

∀

∈ R

: 1

C

(r) = 0]

(29b)

1
1
j=1 p0(r) (cid:33)
−

∑i

p0(xc)

n0
∏
i=1

p0(ri) 1

[

i

=j.ri̸

∀

=rj]

[

xc ∈ C

∀

: 1

C

(x) = 1]

(29c)

1

−

1

−

1

−

This represents the probability of rejecting unique r1 ···
priate renormalization at each step as we remove rejected samples.

rn and accepting xc, with appro-

• Our meta-proposal runs a second adaptive rejection loop, using the rejected samples from
the first loop to further constrain the sampling space. Let s = ((s1, ... , sn1 ), s∗)
, where
si are rejected samples, s∗ is the accepted sample, and r ++ s denotes the concatenation of
the rejected samples from both loops:

∈ S

h(s, r; x)

=

1
n0 + n1 + 1

n0
∏
i=1

(p0(ri) (1

1

C

−

(ri)))

n1
∏
i=1

(p0(si) (1

1

C

−

(si)))

1

[

i

=j.(r++s)i̸

∀

=(r++s)j]

[Def.]

(30a)

p0(ri)

(p0(si) (1

1

C

−

(si)))

1

[

i

=j.(r++s)i̸

∀

=(r++s)j]

r

[

∀

∈ R

: 1

C

(r) = 0] (30b)

p0(s∗) 1

C

·

(s∗) ∏
i
n0
∏
i=1

(s∗) ∏
i
n0
∏
i=1

(s∗) ∏
i
n0
∏
i=1

∑i

∑i

∑i

1
j=1 p0(r ++ s)
n1
∏
i=1
1
j=1 p0(r ++ s)
n1
∏
i=1
1
j=1 p0(r ++ s)
n1
∏
i=1

p0(ri)

p0(si)

p0(ri)

p0(si)

=

1
n0 + n1 + 1

p0(s∗) 1

C

·

=

1
n0 + n1 + 1

p0(s∗) 1

C

·

=

1
n0 + n1 + 1

1

[

i

=j.(r++s)i̸

∀

=(r++s)j]

s(i)

[

∀

∈ S

: 1

s(i)

= 0]

(30c)

(cid:17)

C

(cid:16)

p0(s∗) ∏
i

·

1
j=1 p0(r ++ s)

∑i

1

[

i

=j.(r++s)i̸

∀

=(r++s)j]

s∗

[

∀

∈ S

: 1

C

(s∗) = 1] (30d)

32

̸
̸
̸
̸
̸
̸
̸
Preprint. Under review.

• Our meta-meta-proposal continues the adaptive rejection process, starting with the

previously rejected samples in r already removed from consideration:

j(s; r, x) =

1

n0+n1
∏
(cid:32)
i=n0+1
p0(s∗) 1
C
n0+n1
∏
(cid:32)
i=n0+1
p0(s∗) 1
C
n0+n1
∏
(cid:32)
i=n0+1
p0(s∗) 1

[

=

=

·

·

·

∑i
1
−
(s∗) 1

[

j=1 p0(r ++ s) (cid:33)

∀

=sj∧

si̸∈

r]

i

=j.si̸
1

∑i

j=1 p0(r ++ s) (cid:33)

1

−
(s∗) 1

=sj∧

si̸∈

r]

[

i

∀

=j.si̸
1

1

−
=j.si̸

i

∑i

j=1 p0(r ++ s) (cid:33)

=sj∧

si̸∈

r]

∀

n1
∏
i=1

n1
∏
i=1

n1
∏
i=1

p0(si)(1

1

C

−

(si))

[Def.]

(31a)

p0(si)

p0(si)

s(i)

[

∀

∈ S

: 1

C

(cid:16)

s(i)

= 0]

(31b)

(cid:17)

s∗

[

∀

∈ S

: 1

C

(s∗) = 1]

(31c)

Following Def. 5, and substituting our definitions, we calculate the proper weight. Note
that most terms of p0 and 1
will cancel out, as will most of the renormalization terms. The
key difference from WRS is that when q proposes the accepted value x from the distribution

C

p0(x)
n0
j=1 p0(r)
∑

1

−

, there is no corresponding

1
n0
j=1 p0(r)

∑

1

−

term in the numerator:

w =

=

=

=

=

=

(x)

C
1
1
j=1 p0(r)
−

p(x)h(s, r; x)
q(r, x)j(s; r, x)
(cid:101)
p0(x)1

∏

n0
i=1

1

−

∑i

(cid:18)

1

1

−

∑

1
n0+n1+1
1
n0
j=1 p0(r)
n0
j=1 p0(r)
−
n0 + n1 + 1
ψ0
1

∑

−
n0 + n1 + 1

ψ0
1
−
n + 1

1
n0+n1+1 ·

(
···

)

p0(x)
n0
j=1 p0(r) ·
∑

)

(
···

1

−

(cid:19)

[Def. 5]

(32a)

[Substitution]

(32b)

[Cancellation; AC]

(32c)

[Algebra]

(32d)

[ψ0 =

n0
∑
j=1

p0(r)]

(32e)

[Def. of n]

(32f)

This matches the weight formula in Def. 2, confirming that our adaptive sampling procedure
■
is properly weighted for

p as justified by the RAVI framework.

Proposition 11. If

x, w

⟨

(cid:101)
⟩ ∼

QAWRS, then x is distributed according to p.

Proof. This follows from Prop. 8 and Def. 2.

■

33

̸
̸
̸
Preprint. Under review.

E NumPy Implementations of Samplers (Defs. 1 and 2)

Note that implementations in this section are designed for helping readers develop intuition,
tightly coupling math and code. This is opposed to efficient implementation and numerical
stability, so such examples should be thoroughly adapted for performant use.

Listing 1 Weighted Rejection Sampling (WRS)

6 def wrs(p0, 1

, L):

C
for i in range(L+1):
ni = 0
while True:
x
∼
if 1

p0

(x):

C
if i == 0:
xc = x

break

Z =

ni += 1
L
L+∑L
i=0 ni
xc,
Z

⟩

return
(cid:98)

⟨

18 def wrs(p, cond, M):
n = np.zeros(M+1)
19
for i in range(M+1):
while True:

20

21

22

23

24

25

26

27

28

29

x = sample(p)
if cond(x):

if i == 0:
xc = x

break
n[i] += 1
Zhat = M / (M + n.sum())
return xc, Zhat

(cid:98)

Listing 2 Adaptive Weighted Rejection Sampling (AWRS)

7

8

9

10

11

12

13

14

15

16

17

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

30 def awrs(p0, 1

): # L = 1
C
n, ψ0, r = 0, 0, set()
for i in range(2):
while True:
p0(
x
∼
if 1

x /
∈

· |
(x):

r)

C
if i == 0:
xc = x

break

else:

if i == 0:

ψ0 += p0(x)

r.add(x)

n += 1

ψ0
Z = 1
−
n+1
return
(cid:98)

⟨

xc,

Z

⟩

(cid:98)

46 def awrs(p, cond): # M = 1
47

n, psi0, r = 0, 0, set()
for i in range(2):
while True:

x = cond_sample(p, r)
if cond(x):

if i == 0:
xc = x

break

else:

if i == 0:

psi0 += p[x]

r.add(x)

n += 1

Zhat = (1 - psi0) / (n + 1)
return xc, Zhat

48

49

50

51

52

53

54

55

56

57

58

59

60

61

34

Preprint. Under review.

F Simulation Results of Samplers (Defs. 1 and 2)

In this section, we report empirical results from simulation that illustrate the unbiasedness
as well as favorable variance and runtime of our algorithms.

Figure F.1: Left: The compute budget of Def. 1 (WRS) can trade off runtime and variance.
Def. 2 (AWRS) shows comparable variance to WRS at L = 1, which we find sufficient in
practice. Right: The algorithms described in Def. 1 (WRS; L=1) and Def. 2 (AWRS) are
properly weighted. As we draw increasingly many Monte Carlo samples (M) from each
sampler, the mean absolute error (MAE) in the estimate of Z trends towards 0. Across both
panels, error bars reflect 95% confidence intervals over an outer loop of 100 Monte Carlo
1), a uniform Dirichlet draw of probability vectors with size
iterations, where p0 ∼
· |
V = 1, 000, and 1
; Uniform(0, 1)), a draw from a Bernoulli process of size V with
C ∼
success probability π

Uniform(0, 1).

DV(
BV(

·

∼

Figure F.2: AWRS runtime scales favorably. Here we visualize Monte Carlo (N = 100 per
cell) estimates of expected runtime and “corner-case” behavior. As Z increases, runtime
decreases. As K (the number of tokens conforming to 1
) increases towards V = 1, 000
(the vocabulary size), runtime decreases for AWRS. Note that the scales of Z and K decay
logarithmically towards the corners and the color axis also scales logarithmically. For AWRS,
in the regime where K is small, runtime operates dominantly on Z. As K gets large, runtime
is capped by V

K.

C

−

35

101102E[Callsto1C]10−210−1Var(ˆZ)Runtime-VarianceTradeoffAWRS,L=1WRS,L=1WRS,L=4WRS,L=16WRS,L=64100101102103MonteCarloSamples(M)10−210−1MAE(|ˆZ−Z|)QualityofZestimatorAWRSWRS1248163162125250500750875938969984992996998999K0.9990.9980.99610.99220.98440.96880.93750.8750.750.50.250.1250.06250.03120.01560.00780.00390.0020.001ZWRS1248163162125250500750875938969984992996998999K0.9990.9980.99610.99220.98440.96880.93750.8750.750.50.250.1250.06250.03120.01560.00780.00390.0020.001ZAWRS2510205010020050010002000E[Callsto1C]2510205010020050010002000E[Callsto1C]1248163162125250500750875938969984992996998999K0.9990.9980.99610.99220.98440.96880.93750.8750.750.50.250.1250.06250.03120.01560.00780.00390.0020.001ZWRS1248163162125250500750875938969984992996998999K0.9990.9980.99610.99220.98440.96880.93750.8750.750.50.250.1250.06250.03120.01560.00780.00390.0020.001ZAWRS2510205010020050010002000E[Callsto1C]2510205010020050010002000E[Callsto1C]1248163162125250500750875938969984992996998999K0.9990.9980.99610.99220.98440.96880.93750.8750.750.50.250.1250.06250.03120.01560.00780.00390.0020.001ZWRS1248163162125250500750875938969984992996998999K0.9990.9980.99610.99220.98440.96880.93750.8750.750.50.250.1250.06250.03120.01560.00780.00390.0020.001ZAWRS2510205010020050010002000E[Callsto1C]2510205010020050010002000E[Callsto1C]Preprint. Under review.

G Runtime Analysis

G.1 Runtime Analysis of Def. 1 (WRS)

Proposition 2. The expected runtime of QWRS scales with

( L
Z ).

O

p0 repeatedly, until obtaining a sample such that 1

Proof. Def. 1 performs L + 1 independent rejection sampling loops. In each loop, we sample
xj ∼
Define total runtime as T def= ∑L
samples in the ith loop (some number ni ≥
sample).

i=0 Ti, where random variable Ti denotes the number of
0 of rejected samples, and one final successful

(xj) = 1.

C

Since we sample independently with replacement, each Ti is identically distributed as a
geometric distribution supported on N
(0, 1]. Thus
total runtime is as follows.

1 with parameter Z = Prx

p0 [1

(x)]

∈

∼

≥

C

E[T] =

=

L
∑
i=0
L
∑
i=0

E[Ti]

∞
∑
k=1

k Pr[Ti = k]

∞
∑
k=1
∞
∑
k=1
∞
∑
k=1

k (1

−

Z)k

1Z

−

krk

−

1(1

r)

−

1

rk

−

1
(1

1

−

Z)

−

= (L + 1)

= (L + 1)

= (L + 1)

= (L + 1)

=

L + 1
Z

[Def. 1; Linearity of expectation]

(33a)

[Def. of expectation]

(33b)

[PMF of geom. dist.; Independence]

(33c)

[Let r = 1

Z]

−

(33d)

[Simplify by subtracting like terms]

(33e)

[Sum geom. series; r = 1

Z < 1]

−

L
Z

.

(33f)

(33g)

■

Thus, the computational complexity is

O

(cid:16)
G.2 Runtime Analysis of Def. 2 (AWRS)

(cid:17)

In AWRS, we sample without replacement from initial distribution p0, until we reach a sample
which satisfies the constraint, which is sampled with replacement. This is repeated for a total
of 1 + L successive loops.

To derive a simple expression for the expected total number of samples, Eq. (43), the
following general property will be useful.

. Suppose we are sequentially sampling (with or without replacement) according to

Proposition 12. Let
set
two disjoint sets

A1,
probability that x is in

X

P be any (potentially unnormalized) probability distribution over the discrete
P. For any
A1 ⊔ A2. The

be the first occurrence of an element in
A1 ≺ A2] =

A1 ≺ A2) is Pr[

(cid:101)
A2 ⊆ X
A1 (write

, let x

def=

A

P(

(cid:101)

A

.

P(
A1)
A1⊔A2)
(cid:101)

Proof. The crucial insight is that for a sequence drawn with or without replacement, the
event
A1 ⊔ A2 (denote
.
this element x
A

A1 ≺ A2 is determined by the first occurrence of an element in

), which is independent of the order or weight of elements outside of

A

=

A

(cid:101)

36

Preprint. Under review.

That is, Pr[
which x

A1 ≺ A2] = Pr[x
is sampled.

A

A ∈ A1] = P′(

P′(

A1)
A

) , where P′ is the probability distribution from

In sampling with replacement the distribution P′ = P ∝
In sampling without replacement, let R
point when x

⊂ X
is drawn. Then the sampling distribution is P′(x) ∝ 1
(cid:101)
X \
A1)
) = P(
are not yet rejected by definition, simply P′(
A1)
) =
P(
P′(
A
A

P trivially, and we have our result.
be the set of already-rejected elements at the
R(x)P(x). Since the
■

elements in

A

A

.

A1)
P(
)
P(
A
(cid:101)
(cid:101)

Define total runtime as the total number of samples (both rejected and accepted) in the
algorithm:

T def= T0 + T1 +

+ TL

···

(34)

where T0 is the runtime of the first loop (number of samples up to and including the first
accepted sample), and Ti is the runtime of the ith additional loop, for i
. T is the
number of calls to 1

in the algorithm.

1, ... , L

∈ {

}

C

Applying Prop. 12 in our setting, within the first loop, where we are sampling without
replacement from initial distribution p0, define the following shorthand for the probability
that x

appears in the sequence of samples before any element from the set

.

∈ X \ C

C

πx

def= Pr[x

p0(x)
p0(x) + ∑x′∈C
Note that each πx is strictly less than 1, since by assumption p0 places some nonzero
probability mass on elements in

p0(x)
p0(x) + Z

p0(x′)

≺ C

] =

(35)

=

.

C

G.2.1 Runtime of First Loop: T0

For each x
else 0. The runtime T0 is the number of rejections, plus one accepted trial:

define an indicator random variable Y

∈ X \ C

(0)
x = 1 if x appears in the loop,

Thus the expected runtime is as follows.

T0

def= 1 + ∑
x /
∈C

(0)
Y
x

(36)

E[T0] = 1 + ∑
x /
∈C
= 1 + ∑
x /
∈C
= 1 + ∑
x /
∈C

(0)
E[Y
x

]

Pr[Y

(0)
x = 1]

[Linearity of expectation]

(37a)

[Expectation of indicator RV]

(37b)

πx

[Definition of Yx’s and Prop. 12]

(37c)

G.2.2 Runtime of a Subsequent Loop: Ti

Now consider the ith loop, for i

1.

≥

(

X \ C

) the set of accumulated rejections from previous loops, and ψi

def=
Ri p0(r) the total already-rejected probability mass. Note ψi < 1 by assumption. We
to each

Denote Ri ⊆
∑r
begin the ith loop sampling from a distribution which assigns probability p0(x)
ψi
x

−

∈

1

Ri.

∈ X \

37

Preprint. Under review.

Given Ri, for each x
C ∪
in the current loop, else 0. Then,

∈ X \

(

Ri) define indicator random variable Y

(i)
x = 1 if x appears

(i)
E[Y
x

|

Ri] = Pr[Y

|

(i)
x = 1
p0(x)
ψi
1
−
p0(x)
+ Z
ψi
ψi
1
1
−
−
p0(x)
p0(x) + Z

=

=

= πx

Ri]

[Expectation of indicator RV]

(38a)

[Definition of Y

(i)
x and Prop. 12]

(38b)

[Cancellation of 1

ψi > 0]

−
[Definition of πx]

(38c)

(38d)

(i)
x = 1

Ri] is in fact independent of Ri, and is simply
Importantly, the probability Pr[Y
equal to πx = Pr[Y
Ri). That is, the probability is independent
(
of the previous rejections, and independent of which loop we are in, provided only that x is
not in
, and has not yet been rejected. Note this is equivalent to the probability that x is
rejected in the current loop, given it has not been rejected earlier:

(0)
x = 1], for each x

|
∈ X \

C ∪

C

(39)

(40)

πx.

Pr[x

Ri+1 |

x /
∈

∈

(

C ∪

(i)
Ri)] = E[Y
x

Ri] = πx

|

Thus for any x

∈ X \ C

, the probability that it has not yet been rejected is

Pr[x /
∈

Ri |

x /

∈ C

] =

i
∏
i′=1

Pr[x /
∈

Ri′ |

x /
∈

(

C ∪

Ri′−

1)] = (1

−

πx)i

letting R0

def= ∅ for convenience, and noting Pr[x /
∈

R1 |

x /

∈ C

] = 1

−

(0)
E[Y
x

] = 1

−

Given already-rejected Ri from previous runs, the runtime Ti is the number of rejections,
plus one accepted trial:

Ti

def= 1 + ∑
C∪

x /
∈

(

(i)
Y
x

Ri)

(41)

Using the properties above, we can derive the expected runtime in loop i as follows.

E[Ti] = E [E [Ti |

Ri]]

[Law of total expectation]

(42a)

1 + ∑
x /
C∪
∈

(

Ri)

E

(i)
Y
x

(cid:104)

Ri

|

(cid:105)





[Linearity of expectation, defn of Ti]

(42b)

πx

(x)πx

[Eq. (38)]

(42c)

[Reindexing]

(42d)

(cid:35)

Ri

X \

(x)

Ri

πx

[Linearity of expectation]

(42e)







= E

= E

= E

1 + ∑
x /
C∪
∈

(

Ri)


1 + ∑
x /
∈C
E

(cid:34)

1

1

= 1 + ∑
x /
∈C
= 1 + ∑
x /
∈C
= 1 + ∑
x /
∈C

X \

(cid:105)
Ri]πx

(cid:104)
Pr[x /
∈
πx)iπx

(1

−

[Expectation of indicator RV]

(42f)

[Eq. (40)]

(42g)

We can see from the derivation of E[T0] in Eq. (37) that this expression also holds for i = 0.

38

Preprint. Under review.

G.2.3 Total Expected Runtime

Putting this together, we have that for L

1, the expected total runtime is

≥

E[T] =

L
∑
i=0

E[Ti]

πx)iπx

(1

−

(cid:35)

[Linearity of expectation]

(43a)

[Eq. (42)]

(43b)

πx)iπx

(1

−

[Sum constant; Change summation order]

(43c)

πx

[Partial sum of geom. series]

(43d)

L
∑
i=0
1

L
∑
i=0 (cid:34)

=

1 + ∑
x /
∈C
= 1 + L + ∑
x /
∈C
= 1 + L + ∑
x /
∈C
= 1 + L + ∑
x /
∈C

= 1 + L +

(1

−
1

−
(1

−
(1

1

πx)L+1
πx)
−
πx)L+1

−
−
|X \ C| − ∑

x /

∈C

πx)L+1

(1

−

[πx > 0]

(43e)

(43f)

def= p0(x)
p0(x)+Z

From this we can see that our runtime decays polynomially in L with respect to the πx’s
< 1), and so each additional loop comes at reduced cost. Note
(where, recall, πx
that once all items that do not satisfy the constraint are removed, there is just a single step
per sampling loop. This corresponds to the intuition that as we add subsequent loops of
sampling without replacement, the expected number of samples before success in each loop
decreases as the probability mass is shifted off of already-ruled-out items, and the runtime
approaches linear growth in L.

O

More formally, that is: Considering the behavior as L grows, (1
the runtime scales as E[T] =

(L).

πx)L+1 approaches 0, so

−

For fixed L, the runtime scales in the probabilities πx, as E[T] =
πx), since higher
powers are dominated by the linear term for each πx. [Also, considering the behavior as
πx →
(that is, when the probability mass on items satisfying the
constraint becomes negligible in pairwise comparison to those each of those that don’t), we
again have that (1

0. So the expected runtime approaches 1 + L +

∈ X \ C
πx)L+1

1 for each x

(∑x /
∈C

O

.]

Note, for the special case L = 1, the expected runtime is

−

→

|X \ C|

E[T] = E[T0] + E[T1]

=

1 + ∑
(cid:34)
x /
∈C
= 2 + ∑
x /
∈C

πx

+

(cid:35)
2πx −

(cid:34)
π2
x

39

1 + ∑
x /
∈C

πx(1

πx)

−

(cid:35)

(44a)

(44b)

(44c)

Preprint. Under review.

G.3 Runtime Simulation Comparison

Figure G.1: Analytic solutions to expected runtime are corroborated by empirical simulation.
Here we visualize Monte Carlo (N = 1, 000 per cell) estimates of expected runtime as
a function of Z and K (the number of tokens conforming to 1
) for a dense tiling of a
vocabulary of size V = 10. Note again that AWRS scales more gracefully than WRS.

C

40

1.02.03.04.05.06.07.08.09.0K0.90.80.70.60.50.40.30.20.1ZWRSAnalytic(A)1.02.03.04.05.06.07.08.09.0K0.90.80.70.60.50.40.30.20.1ZWRSMonteCarlo(M)1.02.03.04.05.06.07.08.09.0K0.90.80.70.60.50.40.30.20.1ZWRSEst.Error1.02.03.04.05.06.07.08.09.0K0.90.80.70.60.50.40.30.20.1ZAWRSAnalytic(A)1.02.03.04.05.06.07.08.09.0K0.90.80.70.60.50.40.30.20.1ZAWRSMonteCarlo(M)1.02.03.04.05.06.07.08.09.0K0.90.80.70.60.50.40.30.20.1ZAWRSEst.Error5101520E[Callsto1C]5101520E[Callsto1C]−1.0−0.50.00.51.0M−AM+A5101520E[Callsto1C]5101520E[Callsto1C]−1.0−0.50.00.51.0M−AM+APreprint. Under review.

H Extensions of Def. 2 (AWRS)

H.1 Partial Concurrency

The algorithm described in Def. 2 can be further sped up by executing the rejection loop
concurrently. The key insight is that since we never resample any rejected element, as long
as we encounter rejections, we may concurrently sample-without-replacement (SWOR),
only syncing between loops. The recipe for partially concurrent AWRS is as follows:

x,

Z

⟨

⟩ ∼

QAWRS as

Exp(p0(x)), i.e., independent samples from exponential distri-

(cid:98)

p, concurrent AWRS generates

(cid:101)
butions whose rates are parameterized by p0.

Definition 6. Given an unnormalized target
follows:
1. Sample a collection of keys κx ∼
2. Sort the tokens by their keys, x1, ... , x
(x1), ... , 1
3. Evaluate 1
4. Kill all threads with index > i and wait for threads with index < i.
5. Set x to the min index passing element xj, set n0 to j
6. Mark elements x1, ..., xj
7. Repeat for the next loop, still counting rejections as in Def. 2.
8. Calculate

1 before the next loop.

Z as usual.

such that κx1 ≤ ··· ≤
) concurrently until the first acceptance at xi.

1, and set ψ0 to ∑

(x

κx

|X |

|X |

−

|X |

−

C

C

.

1

j
i=0 p0(x).
−

As an additional minor speed-up, since all exponential perturbations are independent, we
may pre-sample them in (very large) batches and simply stream them as needed in the loop.

(cid:98)

For unnormalized logits log
rather than ascending (Vieira, 2014).

p0, sample keys κx ∼

Gumbel(log

p0) and sort keys descending

H.2 Clipped-Probability Early Stopping

(cid:101)

(cid:101)

steps to find a token, and as more
The algorithm described in Def. 2 may take up to
mass is removed,
Z may get arbitrarily small. In these cases, we risk spending compute on
samples that will likely be dropped by SMC, anyways. Here, we present a strategy for early
stopping in such cases, based on a pair of thresholds on rejected probability mass, one for
each loop. While this speedup can be beneficial in some settings, clipped AWRS is no longer
an exact sampler and may yield dead (

Z = 0) samples.

|X \ C|

(cid:98)

Definition 7. Given an unnormalized target
follows:
1. Select two thresholds 0 < θ0 < θ1 < 1.
2. Sample

(cid:98)

p, clipped AWRS generates

x,

Z

⟨

⟩ ∼

QAWRS as

(cid:101)

(cid:98)

X \

r1, ... , rn0, ...

⟨
distributions on

3. If xc0 was found, generate an additional trace

as follows: draw n0 unique rejections through a sequence of renormalized
⟩
r<i until either obtaining xc0 ∈ C
or ∑
s1, ... , sn1, ...

n0
i=1 p0(ri) > θ0.
, by continuing to sample as above
⟩
⟨
from the remaining not-yet-rejected elements, through an additional n1 new unique rejections
> θ1. Then calculate the standard
until either a token is accepted or ∑
Z = 1
n+1 , where ψ0 = ∑
−

n0
i=1 p0(ri) and n = n0 + n1.

n0
i=1 p0(ri) + ∑

n1
j=1 p0

4. If no xc0 was found before hitting θ0, draw 1 single sample x∗ from the remaining elements after

sj

ψ0

(cid:0)

(cid:1)

r<i.

X \

the first loop
(cid:98)
5. If 1
6. If 1

C
C
but set

Z = (n1+1)(1
n+1

(cid:98)
−

ψ0)

(x∗) = 0, then
(x∗) = 1, then run the second loop as in step 3 through either an acceptance or the θ1 cutoff,

Z = 0.

, where n1 is the number of rejections only in the second loop.

(cid:98)
In summary,

Z def= (n1+1)1[ψ0>θ0](1

n+1

ψ0)

−

From the RAVI perspective (App. B), we may think of clipped AWRS as the following
extension of AWRS (App. B.6):

(cid:98)

41

Preprint. Under review.

Proposal. We again define over the space
, and as before, run an adaptive rejection
loop. However, we now stop either when we see an accepted sample xc0, or when the rejected
samples have total probability mass exceeding θ0. In that case, the returned x is simply the
next proposed value, whether or not it satisfies the constraint.

X × R

Meta-proposal. Given x, we only care when x satisfies the constraint; otherwise, no matter
Z = 0. Assuming x does satisfy the constraint, the meta-proposal
our meta-proposal,
generates its own rejection loop with probability mass bound by θ1, then selects a split point
n0. If the total mass up to the split point exceeds θ0, we clamp n0 to the first index at which
θ0 was exceeded. This allows us to tractably infer the first loop, once again not needing to
actually sample the split point.

(cid:98)

Meta-meta-proposal. We continue generating a second loop with threshold θ1 to propose
the remaining samples from the meta-proposal, which we had stopped at its split point.

42

Preprint. Under review.

I Context-Sensitive Pattern Matching

In order to generate instances of pattern-matching specifications that exceed the expressive-
ness of standard FSM-based approaches, we executed the following procedure:

1. Prompt claude-sonnet-3.7 to generate pattern-matching expressions to test a modern
regex engine that use advanced features including lookaround, backreferences, condi-
tionals, etc. This was repeated until generating 1503 candidates.

2. Drop duplicates.
3. Filter all proposed expressions that do not comply with the regex library (Barnett, 2014).

This confirms these specifications are satisfiable.

4. Filter all proposed expressions that do comply with the interegular library (MegaIng,
2019). This confirms these specifications exceed the capacities of FSM-based approaches.
5. Confirm that the empty string "" is a valid prefix, but not complete. This satisfies our

setting for benchmarking LM generation.

Following these steps, 402 valid test cases remained. The authors manually inspected a sub-
set of the final dataset and found that it exploited a wide range of challenging specifications.
Here are some examples:

• Repeating pattern of two characters in forward then reverse order:

^(\w)(\w)(?:\2\1)+$

• Arbitrarily nested center embedding:

^(<<(?R)*>>|\w+)$

• Conditional matching:

(\d{3})?(?(1)abc\1|xyz)

• A mutually recursive arithmetic expression parser:

(?(DEFINE)(?<expr>(?&term)(?:[+\-](?&term))*)(?<term>(?&factor)(?:[*/](?&factor))*)
(?<factor>\d+|\((?&expr)\)))^(?&expr)$

• A simple JSON parser:

"^(?(DEFINE)(?<json>(?<obj>\{(?:(?&str):(?&val)(?:,(?&str):(?&val))*|)\})
|(?<arr>\[(?:(?&val)(?:,(?&val))*|)\])|(?<str>""(?:[^""\\]|\\.)*"")|(?<val>(?&obj)
|(?&arr)|(?&str)|true|false|null|-?\d+(?:\.\d+)?(?:[eE][+-]?\d+)?)))(?&json)$"

43

Preprint. Under review.

J Experiment Details

This section provides details about the hyperparameters and hardware used in our experi-
ments.

Temperature & Max Tokens. All experiments were run with temp 1.0. For all methods,
we set a maximum number of tokens threshold to prevent excessively long generation times.
Thresholds are set for each domain depending on the typical length of valid sequences from
that domain. The following thresholds were set: Pattern Matching (32), Molecular Synthesis
(40), Goal Inference (100), Text-to-SQL (100), JSON (350).

SMC Resampling.
In our SMC-based methods (Twisted SMC, AWRS-SMC), a resampling
step is triggered when the effective-sample size (ESS) of the particle beam falls below a set
threshold. For the Pattern Matching, Text-to-SQL, and JSON domains, we use an ESS of
M/2 for AWRS-SMC and M
1 for Twisted SMC, where M is the number of particles.
The Twisted SMC threshold was chosen to trigger a resampling step as soon as a particle
has been deemed invalid by the constraint. These domains used standard multinomial
resampling. For the Goal Inference and Molecular Synthesis domains, we use an ESS of
M, which triggers particle resampling at every step. In these cases, we used a stratified
resampling scheme as in (Loula et al., 2025).

−

Hardware. Text-to-SQL, Pattern Matching and JSON experiments were run on a single
L40S GPU, with the exception of any runs using Llama 3.3 (70B), which used 4 L40S GPUs.
Goal Inference and Molecular Synthesis experiments were run on a single 40GB A100 GPU.

44

Preprint. Under review.

K Experiments with Language Models of Varying Size

Method

Base LM
ARS-LCD
TM-LCD

Accuracy

Runtime (sec/ex)

0.159 (0.12, 0.20)
0.953 (0.93, 0.97)
0.950 (0.93, 0.97)

0.04 (0.04, 0.05)
0.07 (0.06, 0.08)
8.45 (6.17, 11.39)

Method

Base LM
ARS-LCD
TM-LCD

0.570 (0.52, 0.62)
0.993 (0.98, 1.00)
0.978 (0.96, 0.99)

Accuracy

Runtime (sec/ex)

0.10 (0.09, 0.11)
0.13 (0.11, 0.14)
6.91 (5.68, 8.46)

0.28 (0.26, 0.30)
0.20 (0.19, 0.22)
0.36 (0.33, 0.40)

Sample-Verify
Twisted SMC
AWRS-SMC

0.373 (0.33, 0.42)
0.452 (0.41, 0.50)
0.974 (0.96, 0.99)

0.20 (0.19, 0.21)
0.11 (0.10, 0.13)
0.29 (0.26, 0.33)

Sample-Verify
Twisted SMC
AWRS-SMC

0.781 (0.74, 0.82)
0.796 (0.76, 0.84)
0.990 (0.98, 1.00)

(a) Llama 3.2 1B

(b) Llama 3.1 8B

Accuracy

Runtime (sec/ex)

Method

Base LM
ARS-LCD
TM-LCD

0.818 (0.78, 0.86)
0.993 (0.98, 1.00)
0.990 (0.98, 1.00)

Sample-Verify
Twisted SMC
AWRS-SMC

0.858 (0.82, 0.89)
0.846 (0.81, 0.88)
0.995 (0.99, 1.00)

(c) Llama 3.3 70B

0.22 (0.21, 0.24)
0.25 (0.23, 0.28)
5.24 (4.50, 6.14)

0.50 (0.46, 0.54)
0.44 (0.41, 0.48)
0.50 (0.46, 0.55)

Table 2: Comparison of method accuracy and runtime across language models of varying
size on the Pattern Matching domain. Confidence intervals bootstrapped at the level of
95%. Runtime represents the average execution time (in seconds) across all instances in the
dataset. Sample-Verify and Twisted SMC were run with M = 10 particles. AWRS-SMC was
run with M = 5 particles. Llama 3.3 8B results are copied from Tab. 1e and repeated here
for convenience. All models are instruct versions.

Figure K.1: Accuracy and runtime of AWRS-SMC and Twisted SMC with varying particle
counts on the Pattern Matching domain, using LMs of different sizes. AWRS-SMC with a
smaller LM achieves better performance than Twisted SMC with larger LMs at the same
runtime cost. All models are instruct versions.

45

0.00.20.40.60.80.29 0.66 1.00 51020401102345510204051020Runtime (seconds per example)AccuracyMethodAWRS SMC w/ Llama 3.2 1BTwisted SMC w/ Llama 3.2 1BTwisted SMC w/ Llama 3.1 8BTwisted SMC w/ Llama 3.1 70B
