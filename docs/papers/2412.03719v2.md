From Language Models over Tokens to Language Models over Characters

Tim Vieira 1 Benjamin LeBrun 2 Mario Giulianelli 1 Juan Luis Gastaldi 1 Brian DuSell 1 John Terilla 3
Timothy J. O’Donnell 4 2 5 Ryan Cotterell 1

Abstract
Modern language models are internally—and
mathematically—distributions over token strings
rather than character strings, posing numerous
challenges for programmers building user appli-
cations on top of them. For example, if a prompt
is specified as a character string, it must be to-
kenized before passing it to the token-level lan-
guage model. Thus, the tokenizer and consequent
processing are very sensitive to the specification
of the prompt (e.g., whether the prompt ends with
a space or not). This paper presents algorithms
for converting token-level language models to
character-level ones. We present both exact and
approximate algorithms. In the empirical portion
of the paper, we benchmark the practical runtime
and approximation quality. Across four publicly
available language models, we find that—even
with a small computation budget—our method is
able to accurately approximate the character-level
distribution at reasonably fast speeds, and that a
significant improvement in the language model’s
compression rate (bits/byte) is achieved.

https://github.com/genlm/genlm-bytes

1. Introduction

Modern language models are engineered as probability
distributions over strings of tokens rather than strings of
characters. However, this leads to a fundamental tension
between the users of language models and the engineers
token-level models are
who build them. Specifically,
rife with unintuitive behaviors that—without a technical
fix—baffle users. As an illustrative example of a common
user complaint, we exhibit the prompt boundary problem
(see below). This paper provides a principled solution to

1ETH Zürich 2Mila 3City University of New York 4McGill Univer-
sity 5Canada CIFAR AI Chair . Correspondence to: Tim Vieira
<tim.f.vieira@gmail.com>.

Proceedings of the 42 nd International Conference on Machine
Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025
by the author(s).

the prompt boundary problem as well as other oddities that
make interfacing with token-level language models with
character-level prompts hard for users.

Tokenized language models: A brief overview.1 Let Σ
be an alphabet of characters, and let Σ∗ denote the set of
all strings that can be built from it. Suppose there is a true
Σ over Σ∗ that we seek to model. We observe a
distribution p∗
training corpus of character strings: σ(1), ... , σ(M ) i.i.d.∼ p∗
Σ.
However, rather than estimating a language model that ap-
proximates p∗
Σ directly, we employ a (possibly stochastic) to-
kenizer τ that transforms the training corpus into a corpus of
token strings. δ(1) ∼ τ (· | σ(1)), ... , δ(M ) ∼ τ (· | σ(M )).
Next, we estimate a token-level language model to fit the
strings δ(1), ... , δ(M ). Lastly, we use p∆ to generate char-
acter strings through the following generative process: (i)
sample δ ∼ p∆ and (ii) return σ = κ(δ) where κ is a de-
coding function. Let pΣ denote the resulting distribution
of this process. Practically, we hope that the choice of τ
and κ should aid in our ability to estimate p∗
Σ in the sense
that p∗
Σ ≈ pΣ. Commonly used tokenizers in the realm of
LLMs use tokenizers τ that break long strings into chunks.
Intuitively, generating chunks instead of individual char-
acters helps because it effectively shortens strings without
obfuscating them using a complicated encoding.

The prompt boundary problem. Consider the case of
GPT2 (Radford et al., 2019), which was trained over token
strings created from byte-pair encoding (BPE; Sennrich et al.
(2016); Gage (1994)). Suppose we wish to generate continu-
ations of the prompt: "In␣the␣kingdom␣of␣the␣blind,␣the.
Unfortunately, the interface to p∆ is not ideal: it does not
accept a character string; thus, it is common to encode it as
a token string with an encoding function τ BPE:2,3

τ BPE("In␣the␣kingdom␣of␣the␣blind,␣the)
11, ␣the
13239 , ␣of
= ["
262 ]

262 , ␣kingdom

262 , ␣blind

286, ␣the

818, ␣the

7770 , ,

1, In

1We adopt the notation of Gastaldi et al. (2025), who gave a general
characterization for when training with tokenization allows for
consistent estimation of the true distribution over characters.
2In practice, τ BPE outputs an integer sequence; we provide the
substring gloss for readability.
3We write δ = τ (σ) when τ is deterministic (i.e., a function).

1

From Language Models over Tokens to Language Models over Characters

If we complete the prompt by taking the most likely next
token (greedy completion), we generate the following:

end of the prompt. (4) Continue generating as usual. Now,
we can see how token healing patches the running example:

530 , -
[␣one

12, eyed

18834, ␣man

582 , ␣is

318, ␣king

5822 , ."
526]

Now that we have our generated output, we can apply the
decoding function κ that maps token strings to character
strings as part of the tokenization protocol:

κBPE(···) = "In␣the␣kingdom␣of␣the␣blind,␣the

␣one-eyed␣man␣is␣king."

This is a good completion, as the string is a well-known
proverb. However, if we tweak the prompt ever so slightly
by inserting a trailing whitespace:

τ BPE("In␣the␣kingdom␣of␣the␣blind,␣the␣)
262 , ␣
11, ␣the
= ["
220]

262 , ␣kingdom

13239 , ␣of

262 , ␣blind

286, ␣the

818, ␣the

7770 , ,

1, In

286, ␣the

389 , ␣seen

995 , ␣are

262 , ␣world

greedy completion gives [ills
2171, ␣of
1775 , ··· ]
because the conditional probability (−→p∆) of generating the
characters we want becomes highly unlikely, dropping from
0.98 = −→p∆(␣one
505 |
τ BPE(··· ␣the␣)) upon appending the ␣ character. Hence, the
trailing white space undesirably impacts the output, which
no longer corresponds to the proverb.

| τ BPE(··· ␣the)) to 5 × 10−7 = −→p∆(one

530

We formalize the prompt boundary problem as fol-
lows: Let σ ∈ Σ∗ be our initial prompt, such as
"In␣the␣kingdom␣of␣the␣blind,␣the). Now, consider a
pair of future strings σ′·α and σ′·β with a common prefix
σ′ (e.g., ␣one and ␣ills) We say a language model interface
suffers from the prompt boundary problem if moving the
common prefix σ′ (e.g., ␣) across the boundary into the
prompt does not preserve the relative probability of the pair
of future strings, i.e., the relative probability of σ′·α and
σ′·β given σ does not equal the relative probability of α
and β given σ·σ′. Our example fails this test because the
probability of ␣one was much higher than ␣ills, but then,
the relative order swaps after moving ␣ into the prompt.

Our perspective is that the prompt boundary problem arises
from incorrectly conditioning the token-level language
model on a string of characters by using τ (σ) rather than
finding token strings that best match the prompt σ.4

(cid:44)→ The token healing heuristic: Lundberg & Ribeiro (2023)
present token healing as a heuristic that mitigates the prompt
boundary problem; it works as follows: (1) Tokenize (i.e.,
encode) the prompt. (2) Backup the tokenized prompt to the
penultimate token. (3) Generate the next token subject to the
constraint that it starts with the unmatched substring at the

τ BPE("In␣the␣kingdom␣of␣the␣blind,␣the␣)
11, ␣the
= ["

262 , ␣kingdom

13239 , ␣of

262 , ␣blind

286, ␣the

818, ␣the

7770 , ,

1, In

(cid:1)(cid:1)␣
262 , (cid:1)
220]

␣one
The most probable next token starting with ␣ is
530 . Now,
generating the remaining tokens recovers the desired output,
as it matches the prompt before we added the whitespace.

Unfortunately, backing up one token is insufficient for the
general case, as the following example will illustrate. Con-
sider generating from GPT2 using the prompt Hello,␣worl:

τ BPE(Hello,␣worl) = [Hello

15496, ,

11, ␣wor

476 , l
75]

likely next character ought

The most
to be d as
Hello,␣world is a common expression (popularized in edu-
cational material). However, the most likely token results
in Hello,␣worlwide, an apparent misspelling of worldwide.5
Unfortunately, token healing’s strategy of backing up by a
l
single token cannot salvage the poor tokenization as
75 is
still the most common next token that is consistent with the
l
75, we are back where we
string l. Thus, after generating
75, wide
476 , l
11, ␣wor
started, generating [Hello
4421].
(cid:44)→ Getting it right: A simple “probability 101” expression
tells us the correct solution to the prompt boundary prob-
lem. Consider a ∆∗-valued random variable Y , distributed
according to p∆. Then, the correct way to sample from p∆
conditioned on a character string σ is according to

15496, ,

p∆|Σ(δ | σ) def= P

Y ∼p∆

[Y = δ | κ(Y ) ⪰ σ]

(1)

where we have conditioned on the event that the decoded
string κ(Y ) has σ as a prefix (i.e., κ(Y ) ⪰ σ). While
innovative with respect to the literature, the expression
PY ∼p∆ [Y = δ | κ(Y ) ⪰ σ] conveys the probability we are
interested in precisely and concisely. For the more procedu-
rally minded, this corresponds to the following process:

1 def conditional_token_generation(σ):

2

3

4

while True:

sample δ ∼ p∆
if κ(δ) ⪰ σ: return δ # accept

This is, of course, very inefficient, but fear not—we will
provide an equivalent, efficient algorithm.

Our method finds a set of token strings that form a covering,
a key technical concept we introduce in this paper. We will
provide the precise definition in due course; for now, we
will illustrate the covering of Hello,␣worl:

4Probabilistic condition preserves the necessary relative probabili-
ties that we specified in the prompt boundary problem.

5The misspelling is a testament to the extent to which the tokenized
prompt is out-of-distribution.

2

From Language Models over Tokens to Language Models over Characters

(0.9820714) : [

(0.0106702) : [

(0.0070749) : [

(0.0000830) : [

(0.0000369) : [

(0.0000225) : [

(0.0000179) : [

Hello
15496,
Hello
15496,
Hello
15496,
Hello
15496,
Hello
15496,
Hell
28254,
Hello
15496,

,
11,
,
11,
,
11,
,
11,
,
11,
o
78,
,
11,

␣world
995 ]
␣worlds
11621 ]
␣worldwide
8688
␣worldly
43249 ]
␣worldview
29081
␣world
995 ]
l
75]

,
11,
␣wor
476 ,

]

]

.
.
.

Notice that each token string in the covering has the char-
acter string Hello,␣worl as a prefix (after decoding), but
it may have some extra characters (marked by underlining)
in the partially matched last token—just like token healing.
The size of the complete covering may, in general, be ex-
ponential in the length of the character string. However, we
can enumerate its high-probability members very quickly
in practice. Since the worst-case time to find the top-K
elements of the covering might be exponential, we provide
a more aggressive approximation based on beam search that
gives a good approximation even with small beam sizes.
Unlike token healing, the sequence [Hello
995 ] is the
highest probability member of the covering; thus, it is not
pruned away by the aggressive heuristics based on τ , as in
token healing. The covering is defined only in terms of κ,
and the pruning is based on the language model probability,
which typically prioritizes the token strings that are most
reflective of the language model’s training data. We provide
an algorithm for correctly conditioning a token-level model
on a character string in §3.4.

11, ␣world

15496, ,

Character-level model. At the character level, working
with the language model is intuitive. Consider, again,
our example illustrating the prompt boundary problem.
Appending whitespace behaves predictably, as it satisfies
the probabilistic chain rule:
−→pΣ(␣one | "In␣the␣kingdom␣of␣the␣blind,␣the)

= −→pΣ(␣ | "In␣the␣kingdom␣of␣the␣blind,␣the)

· −→pΣ(one | "In␣the␣kingdom␣of␣the␣blind,␣the␣)
Here, −→pΣ denotes the character-level model’s conditional dis-
tribution. Similarly, recall the Hello,␣world example above.
The character-level model correctly infers that d is the most
likely next character given Hello,␣worl. The computation
of this conditional probability is simply the total probability
of the covering of Hello,␣world divided by the total prob-
ability of the covering of Hello,␣worl. These quantities are
derived from our concept of covering, which directly leads
to an algorithm for determining the distribution over possi-

ble next characters. Beyond the prompt boundary problem,
computing the conditional probability of a character string
given a token-level language model has many applications.

Applications. Aside from making character-level condi-
tioning well-behaved, we highlight a few applications of
language models requiring careful character-level reasoning:

(cid:44)→ Character-level constraints: Enforcing character-level
constraints on allowed strings is a promising area that has
received much recent attention (e.g., Scholak et al., 2021;
Poesia et al., 2022; Geng et al., 2023; Microsoft, 2023;
Willard & Louf, 2023; Koo et al., 2024; Loula et al., 2025).

(cid:44)→ Computational psycholinguistics: Computing the con-
textual surprisal (negative log probability) of a character
substring to predict human reading times (Hale, 2001; Levy,
2008). Two recent papers (Oh & Schuler, 2024; Pimentel
& Meister, 2024) have given algorithms for computing the
surprisal of whitespace-separated words under a number of
strong assumptions. Our algorithms can compute the contex-
tual surprisal of arbitrary character strings. Giulianelli et al.
(2024) show experimentally that having the flexibility to
compute character substring surprisals leads to a more pre-
dictive model of reading time than a fixed notion of a word.

Does it work? In the experimental portion of our paper
(§4), we report the empirical runtime of our algorithm for
converting token-level language models to character-level
ones and quantify its accuracy in estimating the conditional
distribution over characters. We find that even with a lim-
ited computational budget, our method provides an accurate
estimate of the conditional distribution over the next char-
acter under four publicly available language models.6 We
also find that the compression rate (bits/byte) is significantly
improved by estimating the probability of the corpus as a
character string rather than a canonical token string.

2. Background

2.1. Alphabets and Strings

An alphabet Γ is a non-empty, finite set of elements called
symbols. A string γ over alphabet Γ is a finite sequence
γ = γ1 ··· γN for some 0 ≤ N < ∞ of symbols where
γ1, ... , γN ∈ Γ. Let |γ| denote the string’s length N . We de-
note the empty string as ε. For any alphabet Γ, let Γ∗ denote
the set of all strings over Γ, and let Γ+ denote the set of all
non-empty strings over Γ. For any two strings γ′, γ′′ ∈ Γ∗,
we denote their concatenation as γ′·γ′′. Additionally, we
define S·S′ def= {γ·γ′ | γ ∈ S, γ′ ∈ S′} for any S, S′ ⊆ Γ∗.
Given a string γ such that |γ| ≥ t, let γ<t denote the string
made from the first t−1 characters of γ. We write γ ⪯ γ′

6Specifically, Llama-3.2-1B, Meta-Llama-3.1-8B, DeepSeek-R1-
Distill-Llama-8B, and phi-4 (14B).

3

From Language Models over Tokens to Language Models over Characters

if γ is a prefix of γ′ and γ ≺ γ′ if γ is a proper prefix of γ′.
The relation ⪯ defines a partial order on Γ∗. We write ⪰
and ≻ to refer to the relations ⪯ and ≺ with their respective
arguments transposed.

2.2. Language Models and Prefix Probability

A language model pΓ is a probability distribution over
Γ∗ where Γ is an alphabet. Let Y be a Γ∗-valued random
variable distributed according to pΓ and γ ∈ Γ∗. The prefix
probability −→pΓ(γ) is the probability that Y has prefix γ:

−→pΓ(γ) def= P

Y ∼pΓ

[Y ⪰ γ] =

(cid:88)

1{γ′ ⪰ γ} pΓ(γ′)

(2)

γ′∈Γ∗

We also define the following shorthand for the conditional
prefix probability −→pΓ(γ′ | γ) as the probability of the event
Y ⪰ γ·γ′ provided that Y ⪰ γ:

−→pΓ(γ′ | γ) def= P

Y ∼pΓ

[Y ⪰ γ·γ′ | Y ⪰ γ] =

−→pΓ(γ·γ′)
−→pΓ(γ)

(3)

We may express the probability of γ as a product of condi-
tional prefix probabilities:

pΓ(γ) = −→pΓ(EOS | γ)

|γ|
(cid:89)

t=1

−→pΓ(γt | γ<t)

(4)

where each −→pΓ(γt | γ<t) is an instance of Eq. (3), and

−→pΓ(EOS | γ) def=

pΓ(γ)
−→pΓ(γ)

(5)

Here, EOS is a distinguished end-of-string symbol that can-
not appear in any alphabet. Of particular interest are the
single-symbol conditional prefix distributions −→pΓ(· | γ<t),
as they may each be interpreted as a probability distribu-
tion over the set Γ ∪ {EOS}—in fact, modern language
models7 are defined via the product in Eq. (4) where each
single-symbol conditional prefix probability comes from the
learned parametric model.8

2.3. Tokenization

We now discuss our basic formalization for tokenization.

Definition 1. An (exact) tokenization model is a tuple
(Σ, ∆, τ , κ) where

• Σ is an alphabet of character symbols

• ∆ is an alphabet of token symbols
• τ is a (possibly) stochastic encoder: τ (· | σ) is a proba-

bility distribution over ∆∗ for each σ ∈ Σ∗

(cid:80)

• κ : ∆∗ → Σ∗ is a decoder function satisfying exactness
δ∈∆∗ 1{κ(δ) = σ}τ (δ | σ) = 1, for all σ ∈ Σ∗
Definition 2. A tokenized language model pΣ is a language
model over Σ∗ that is parameterized by a language model
p∆ over ∆∗ and a decoding function κ : ∆∗ → Σ∗. This
tokenized language model generates character strings via
the following process: (i) δ ∼ p∆, (ii) σ ← κ(δ). Thus, the
character strings σ generated have the distribution:

pΣ(σ) def= P

Y ∼p∆

[κ(Y ) = σ]

(6)

Note that pΣ(σ) accounts for the fact that many token
strings may be associated with a given character string
through κ.9
To describe that association, we define
E(σ) def= {δ ∈ ∆∗ : σ = κ(δ)}, the set of encodings for
any character string σ ∈ Σ∗.10

What about τ ? The reader may notice that τ does not
appear in Eq. (6). Although τ is essential for generating
training data, once the model p∆ has been trained, the in-
formation in τ is not of immediate practical use. Moreover,
attempts to leverage τ seem to lead to faulty heuristics, as we
discussed in the introduction. We note that under exactness,
τ (σ) must be present in E(σ). This is because exactness im-
plies that E(σ) ⊇ {δ ∈ ∆∗ : τ (δ | σ) > 0} for all σ ∈ Σ∗.
In the common case where τ is deterministic11 we empha-
size that E(σ) ⊇ {τ (σ)}. The tokenization model would
need to be bijective12 for E(σ) = {τ (σ)}. Unfortunately,
common tokenizers (e.g., BPE) are not bijective.

The mirage of the canonical tokenization. Consider
the case when the encoder τ is deterministic. In that case,
we write δ = τ (σ), and we call this δ the canonical
tokenization of σ. Note that even if τ is deterministic, there
may exist many noncanonical tokenizations δ′ ∈ E(σ)
such that δ′ ̸= τ (σ) with nonzero probability p∆(δ′) > 0.
Thus, the character string generation process includes a
mix of canonical and noncanonical token strings—making
it incorrect to only consider a character string’s canonical
tokenization when assessing its probability. In practice, the
conditional probability PY ∼p∆ [Y = δ | κ(Y ) = σ] over
the encodings δ of a character string σ tends to be highly

7E.g., transformers (Vaswani et al., 2017), RNNs (e.g., Mikolov
et al., 2010), and n-gram models (e.g., Shannon, 1948).
8The reader may notice that the equations for pΓ(γ), −→pΓ(γ′ | γ),
and −→pΓ(EOS | γ) are mutually recursive. Thus, they may appear
circular. The key to resolving this concern is to recognize that pΓ
is given as a base case. We note, however, that some readers may
view Eq. (4) as the definition of the language model pΓ, taking
the components on the right-hand side of Eq. (4) as the base case.

9Many authors (Cao & Rimell, 2021; Chirkova et al., 2023; Phan
et al., 2024) have discussed this particular complication; however,
no algorithms for computing the next-character distribution exist.
10Note that |E(σ)| can be very large, e.g., infinite in the worst case.

In the case of BPE, it is exponential in |σ|.

11I.e., τ (δ | σ) ∈ {0, 1} for all δ ∈ ∆∗, σ ∈ Σ∗.
12I.e., a bijective tokenizer is deterministic, exact, and satisfies

τ (κ(δ)) = δ for all δ ∈ ∆∗.

4

From Language Models over Tokens to Language Models over Characters

concentrated on the canonical tokenizations, as illustrated
in Example 1 below.

Example 1. Below, we show GPT2’s top encodings from
E(Hello,␣world), ranked by their conditional probability.

␣world
995 ]

This short string has
78 encodings from
numerous partitions
of Hello,␣world into
substrings of the
tokenization
alphabet ∆. We see
that the probability
heavily concentrates
on the top string,
which is canonical.

(0.9999719) : [

(0.0000229) : [

(0.0000024) : [

(0.0000017) : [

(0.0000004) : [

(0.0000002) : [

(0.0000002) : [

Hello
15496,
Hell
28254,
Hello
15496,
He
1544,
H
39,
Hello
15496,
H
39,

,
11,
o
78,
,
11,
llo
18798,
o
ell
695,
78,
,
11,
ello
11109,

,
11,
␣wor
476 ,
,
11,
,
11,
␣w
266,
,
11,

␣world
995 ]
ld
335]
␣world
995 ]

␣world
995 ]

orld
1764]
␣world
995 ]

A character-level interface. A character-level interface
to the token-level language model p∆ is available in the
following equations, which hold ∀σ, σ′ ∈ Σ∗:

−→pΣ(σ) = P

Y ∼p∆

[κ(Y ) ⪰ σ]

−→pΣ(σ′ | σ) =

−→pΣ(EOS | σ) =

−→pΣ(σ·σ′)
−→pΣ(σ)
pΣ(σ)
−→pΣ(σ)

(7)

(8)

(9)

These equations show that we can have a complete character-
level language model derived from the tokenized language
model if we can compute—or approximate—the necessary
summations implied by Eq. (6) and (7); specifically,

pΣ(σ) =

−→pΣ(σ) =

(cid:88)

δ∈∆∗
(cid:88)

δ∈∆∗

1{κ(δ) = σ} p∆(δ)

1{κ(δ) ⪰ σ} p∆(δ)

(10)

(11)

We will develop effective methods for these summations for
the family of strict-prefix monotone decoders κ (described in
§2.4) where Eq. (10) and Eq. (11) admit a finite summation.

2.4. Key Properties of κ

This section defines the essential properties of κ we require.
Definition 3. We say κ : ∆∗ → Σ∗ is
• prefix monotone if δ ⪯ δ′ =⇒ κ(δ) ⪯ κ(δ′)
• strict-prefix monotone if δ ≺ δ′ =⇒ κ(δ) ≺ κ(δ′)

In simpler terms, strict-prefix monotonicity implies that con-
catenating a token to the encoding necessarily concatenates
at least one character to the decoded character string. The
diagrams below illustrate how GPT2’s strict-prefix mono-
tone κ gives rise to a certain alignment between three token
strings and the character string Hello,␣world:

5

Hello,␣world

Hello,␣world

Hello,␣world

Hello
15496

,
11

␣world
995

H
39

ell
695

o
,
11
78

␣world
995

Hello
15496

,
11

␣wor
476

l
d
67
75

More formally, every application σ1 ··· σM = κ(δ1 ··· δM )
of a strict-prefix monotone mapping has the following prop-
erties. Each token in δ1 ··· δM maps to one or more contigu-
ous characters in σ1 ··· σM . Moreover, the mappings do not
exclude any characters, and no edges of the mapping cross
one another. Strict prefix monotonicity, in contrast to prefix
monotonicity, ensures that there are no deletions of tokens in
the mapping, i.e., each token maps to at least one character.

Strict-prefix monotonicity is the key structural property re-
quired by §3’s algorithms, as it allows us to replace an infi-
nite sum with a finite sum in Proposition 1. We briefly men-
tion an important special case. A decoder κ is multiplica-
tive if κ(δ1 ··· δN ) = κ(δ1) ··· κ(δN ) for all δ1 ··· δN ∈ ∆∗,
and non-erasing if κ(δ) = ε =⇒ δ = ε. If κ is multi-
plicative, it is prefix monotone; if it is also non-erasing, it
is strict-prefix monotone. Both BPE and WordPiece are
multiplicative and non-erasing.

3. Algorithms
This section gives algorithms for computing pΣ(σ), −→pΣ(σ),
−→pΣ(σ′ | σ), −→pΣ(EOS | σ), and conditional token generation.
We assume throughout that κ is strict-prefix monotone.

3.1. Covering

Eq. (11) shows that we can, in principle, compute the prefix
probability −→pΣ(σ) by summing over prefix-encodings
of σ, P(σ) def= {δ ∈ ∆∗ : κ(δ) ⪰ σ}. Although P(σ) is
infinitely large, we can exploit the prefix monotone structure
of κ to find a different way to perform the summation by
summing over a finite set. Let δ ∈ ∆∗, δ1 ··· δM = δ, and
σ ∈ Σ∗. We say that δ covers σ if and only if κ(δ) ⪰ σ.
Monotonicity ensures that for all δ ∈ P(σ), we have
that ∀δ′ ∈ ∆∗ : κ(δ·δ′) ⪰ σ.
In other words, any δ
that decodes to an extension of σ (i.e., κ(δ) ⪰ σ) will
continue to do so if we append tokens to it. Thus, we may
additionally qualify the relationship as δ minimally covers
σ if additionally κ(δ1 ··· δM −1) ≺ σ. With that in mind,
we define ϕσ(δ) as the shortest prefix δ′ ⪯ δ such that
κ(δ′) ⪰ σ, i.e., ϕσ maps any δ that covers σ to a (possibly
equal) token string that minimally covers σ. Next, we
define the set of minimal prefix encodings of σ, which
we call the covering of σ, C(σ) def= {ϕσ(δ) | δ ∈ P(σ)}.
A more convenient expression for the covering C(σ) of a

From Language Models over Tokens to Language Models over Characters

string σ ∈ Σ∗ is equal to the following subset of ∆∗:

C(σ) =

{ε}
{δ1 ··· δM ∈ ∆+ :

if σ = ε
otherwise

(12)

κ(δ1 ··· δM −1) ≺ σ ⪯ κ(δ1 ··· δM )}






Example 2.

Recall the covering of the string
σ = Hello,␣worl from the intro-
duction. We have repeated it on
the right and couched it in our ter-
minology. Note that the complete
covering C(Hello,␣worl) contains
36,608 token strings; we only show
the top strings according to their re-
spective −→p∆. Below, we list several
properties and observations about
the structure of the covering:

Hello
15496,

[

,
11,

␣world
995 ]

Hello
15496,

[

,
11,

␣worlds
11621 ]

Hello
15496,

[

,
11,

␣worldwide
8688

]

Hello
15496,

[

,
11,

␣worldly
43249 ]

Hello
15496,

[

,
11,

␣worldview
29081

]

Hell
28254,

[

o
78,

,
11,

␣world
995 ]

Hello
15496,

[

,
11,

␣wor
476 ,

l
75]

• The covering for any given string σ always has the prop-
erty that each non-empty token string δ in the covering
decodes to a string that has σ as a prefix, i.e., σ ⪯ κ(δ).
This is illustrated by the gloss string in the tokenization.
• Note that δ may include a partially matched token at
its end (i.e., δM in Eq. (12)). We have marked the extra
characters by underlining them. We note that the 7th
member does not have a partially matched last token.
• Each token string in the covering has at most one par-
tially matched token thanks to the condition κ(δ) ≺ σ ⪯
κ(δ·δ). The 7th member of the cover has a completely
matched last token; hence, there is no underlining.

• We see that if we were to extend any member δ ∈ C(σ)
with an arbitrary string of additional tokens δ′, it would
continue to decode to a string such that κ(δ·δ′) ⪰ σ.
Moreover, δ is minimal (i.e., ϕσ(δ) = δ).

The notion of a covering is used to derive an algorithm
for computing character-level probabilities given a token-
level language model. We first show how it gives us the
prefix probability and subsequently give equations for the
remaining quantities of the character-level language model.

Proposition 1. Suppose (Σ, ∆, τ , κ) is a tokenization model
where κ is strict-prefix monotone and p∆ is a token-level
language model. Then, the prefix probability −→pΣ(σ) for the
character-level model Eq. (6) is given by

−→pΣ(σ) =

(cid:88)

−→p∆(δ),

∀σ ∈ Σ∗

(13)

|C(σ)| is finite for all σ ∈ Σ∗. Bear in mind that the cov-
ering’s size is likely too large to be practical, as there may
still be a large number of summands; however, the set of
high-prefix-probability elements of the covering tends to be
reasonably small, an observation that we verify in §4, and
leverage to develop practical algorithms in §3.

Lastly, we note that the covering contains the set of en-
codings, i.e., E(σ) ⊆ C(σ); hence, the encodings may
be extracted from the covering as follows: E(σ) =
{δ ∈ C(σ) : κ(δ) = σ}. We may also express the prob-
ability of σ in terms of the covering:

pΣ(σ) =

(cid:88)

1{κ(δ) = σ} p∆(δ)

(14)

δ∈C(σ)

3.2. Algorithms for −→pΣ(σ) and pΣ(σ)

The enumeration algorithm will enumerate elements of
the covering along with their prefix probability (for con-
venience). It filters prefixes of token strings that cannot
eventually cover the target string σ. The strict-prefix mono-
tonicity property is essential for this filtering.

Our algorithm enum_cover performs recursive enumera-
tion of the members of the covering C(σ) along with some
metadata. Specifically, the algorithm returns a collection
of triples where each triple (p′, σ′, δ′) satisfies δ′ ∈ C(σ),
p′ = −→p∆(δ′), and σ′ = κ(δ′).

5 def enum_cover(σ1 ··· σN ):
6

if N = 0: return [(1, ε, ε)] # base case
out ← []
for (p′, σ′, δ′) in enum_cover(σ1 ··· σN −1):
# extend

if |σ′| < N :

for δ′′ ∈ ∆:

σ′′ ← κ(δ′·δ′′)
if σ′′

N = σN : # filter

out.append((p′ · −→p∆(δ′′ | δ′), σ′′, δ′·δ′′))
N = σN : # filter character matches

elif σ′

out.append((p′, σ′, δ′))

return prune(σ1 ··· σN , out)

7

8

9

10

11

12

13

14

15

16

Note that this method has an additional parameter, the func-
tion prune, which is used on the last line. This method, as
the name suggests, is used to limit the size of the covering
to prevent excessive growth. We will discuss this parameter
shortly. For now, consider the following definition:

17 def prune_nothing(σ1 ··· σN , out):
18

return out

δ∈C(σ)

Proof. See App. B.

From the output of the enumeration algorithm, we can com-
pute the other key objects and quantities (i.e., C(σ), −→pΣ(σ),
E(σ), pΣ(σ)) in the character-level interface.

■

Eq. (13) is a substantial improvement over Eq. (7) for com-
puting −→pΣ(σ). Specifically, we now have a finite sum, as

Time and space complexity. To meaningfully discuss its
running time, we assume the following:

6

From Language Models over Tokens to Language Models over Characters

• κ(δ′·δ′′) can be evaluated in constant time given κ(δ′).
• the cost of evaluating −→p∆(δt | δ<t) is constant given that

−→p∆(δs | δ<s) has been computed for 0 ≤ s < t.13

the

time

these

running

assumptions,

Under
of
enum_cover(σ1 ··· σN ) can be exponential in N when
no pruning is used. We provide detailed bounds on the
covering’s size in App. C. It is straightforward to verify
that the space complexity is O(|C(σ)|), and the running
time is O(|∆| · (cid:80)|σ|
t=1 |C(σ<t)|) which is dominated by the
|∆| · |C(σ)| term; thus, O(|∆| · |C(σ)|).

Pruning. We now consider some useful pruning heuristics
for the algorithm, which make it an approximation but sub-
stantially improve its running time. We propose a heuristic
based on beam search. This heuristic is very effective: it
gives us a linear running time as a function of the charac-
ter string’s length. It has a parameter K that controls the
approximation quality. Larger K makes the approximation
more accurate, and the approximation becomes exact as K
approaches the size of the (largest intermediate) covering.
We take K to be a global variable in the pseudocode.

(cid:44)→ Our pruning heuristic: Our pruning heuristic enumerates
≤ K distinct token strings modulo their last token. This
choice allows up to |∆| versions of the last token to be
enumerated (if it is not completely matched). Thus, the work
done at each step is O(K · |∆|), and the size of the pruned
list is at most that size. Therefore, the overall running time
is O(N · K · |∆|) for a character string of length N .14

19 def prune_top_K_buckets(σ1 ··· σN , results):
20

buckets ← {}
for item in results:

(p, σ′, δ1 ··· δM ) ← item
# Exclude a partially matched last
key ← δ1 ··· δM −1 if |σ′| > N else δ1 ··· δM
buckets[key].append(item)

pruned ← []
for bucket in buckets.top(K): # by prob.

for item in bucket:

pruned.append(item)

return pruned

21

22

23

24

25

26

27

28

29

30

Bundled beam summing implementation. App. D de-
scribes an implementation strategy that improves the con-
stant factors associated with the pseudocode above. The
key idea is to group the token sequences that fall into the
same bucket in prune_top_K_buckets into a bundle that
represents them compactly. In particular, we can use a trie

13In the case of the common transformer language model (Vaswani
et al., 2017), this can be achieved with efficient caching and
limiting context windows to a constant size.

14Note: finding the (unordered) set of top-K elements from a set S
is possible in O(|S|) time via the median-of-medians algorithm.

to efficiently filter out the next tokens that disagree with the
next character. We can regard the trie as a local language
model that generates the next token character-by-character
according to the probability assigned by −→p∆(· | δ). Each
bundle can be unbundled (if necessary) into the respective
tuples that the enum_cover algorithm maintains.

3.3. Algorithms for −→pΣ(· | σ)

This section gives algorithms for computing the character-
level conditional prefix probability. Recall the definition
of the character-level conditional prefix probability, that is,
Eq. (8) and (9), can be computed from a certain ratio of calls
to −→pΣ (and pΣ in the case of EOS). From here, Eq. (8) and (9)
give a straightforward algorithm for computing the distribu-
tion over Σ ∪ {EOS} given σ ∈ Σ∗. However, a direct trans-
lation would perform duplicate work. Therefore, we provide
the following version, which reuses work between the calls
to −→pΣ than directly evaluating those equations would do.

31 def next_character_probability(σ):
32 N ← |σ|; Z ← 0; p ← {σ′ : 0 for σ′ ∈ Σ∪{EOS}}
33

for (p′, σ′, δ′) ∈ enum_cover(σ):

34

35

36

37

38

39

40

41

42

Z += p′
if |σ′| = N :

# i.e., σ′ = σ

p(EOS) += p′ · −→p∆(EOS | δ)
for δ′′ ∈ ∆:
# extend

σ′′ ← κ(δ′·δ′′)
p(σ′′

N +1) += p′ · −→p∆(δ′′ | δ′)
# i.e., σ′ ⪰ σ

else:
p(σ′
return p/Z

N +1) += p′

# Z = −→pΣ(σ)

3.4. Conditional Generation p∆|Σ(δ | σ)

This section gives a simple algorithm for correctly generat-
ing a token string Y that has a given character-level prompt
σ as its prefix. This algorithm is equivalent to the algorithm
in the introduction but significantly faster.

The algorithm works by enumerating the covering C(σ),
drawing a token string from it in proportion to its prefix
probability, and finishing the token string by sampling a
completion, which can be done from the token-level model.

43 def conditional_token_generation(σ):

δ′ ∼ Categorical({δ′ : p′/−→pΣ(σ)

for (p′, _, δ′) in enum_cover(σ)})

return sample_completion(δ′)

44

45

46

47 def sample_completion(δ′):
48

δ′′ ← ε
while True:

δ ∼ −→p∆(· | δ′·δ′′)
if δ = EOS: break
δ′′ ← δ′′·δ
return δ′·δ′′

49

50

51

52

53

7

From Language Models over Tokens to Language Models over Characters

(a) Error (JSD/byte) vs. speed (bytes/sec). Error is computed between the character-level conditional distributions with beam sizes
K ∈ {2, 4, 8, 16, 32, 64} and a reference distribution computed using a much larger value of K = 128.

(b) Surprisal (bits/byte) vs. speed (bytes/sec). The dotted line shows the canonically tokenized baseline’s surprisal.

Figure 1: Experimental results. Averages are computed over the first 4000 bytes of the wikitext-103-v1 test set. Error
bars denote bootstrapped 95% confidence intervals. See App. A for the full table of results.

The following proposition establishes correctness:

Proposition 2. conditional_token_generation(σ) gen-
erates samples according to p∆|Σ(· | σ) for all σ ∈ Σ∗.
Proof. See App. E.

■

We also note the following corollary, as it gives an inter-
pretation for the categorical distribution in the efficient
conditional_token_generation algorithm.
Corollary 1. For all σ ∈ Σ∗, δ ∈ ∆∗,
−→p∆(δ)
−→pΣ(σ)

[ϕσ(Y ) = δ | κ(Y ) ⪰ σ] =

1{δ ∈ C(σ)} (15)

P
Y ∼p∆

Thus, we have provided an efficient solution to the prompt
boundary problem. We also note that generating from pΣ
a character at a time is also a correct solution to the prompt
boundary problem; however, it is slower, as it does not ben-
efit from the fact that the generated string is shorter in token
space. This is because once the minimally covering token
string has been sampled, the method sample_completion
will generate a complete sequence more efficiently than the
character-at-a-time sample algorithm.

4. Experiments

This section investigates our algorithm’s running time and
accuracy. We use the following setup:

• We use the following publicly available models: Llama-
3.2-1B, Meta-Llama-3.1-8B, DeepSeek-R1-Distill-
transformers
Llama-8B, and phi-4 (14B) from the
library (Wolf et al., 2020). Each model was trained over

8

token strings created from byte-pair encoding (BPE;
Sennrich et al. (2016); Gage (1994)).15

• We use the wikitext-103-v1 corpus as a source of char-
datasets

acter strings; we used the version in the
library. Specifically, we use the test portion.

• We use the

library16 with the
(Kwon et al.,
2023) backend to perform the efficient, batched evaluation
of transformer language models on GPUs. We batch-
evaluate all sequence extensions. Experiments were run
on an L40S GPU with 40GB of memory.

• Our implementation utilizes a trie to efficiently represent
all items in each bucket (see App. D), and the bucket-
based pruning heuristic (§3).

To better understand the quality of the approximation our
method provides, we perform the following experiments:17

• We measure the approximation error as the average
Jensen–Shannon distance (JSD) to a reference model’s
conditional distribution over the next byte (Fig. 1a). We
use a large beam K = 128 as a reference model.

• We evaluate the average surprisal (− log2 probability)
of our model’s estimated conditional distribution over the
next byte in the corpus. As a baseline, we use the average
surprisal (bits/bytes) of the canonical tokenization under
the token-level language model (Fig. 1b).

15Note that in this section we use bytes as our set of characters to

be compatible with the byte-pair encoding algorithm.

16https://github.com/genlm/genlm-backend
17Some additional baselines and experimental results are included

in appendices (F and G).

From Language Models over Tokens to Language Models over Characters

Discussion. As expected, we observe that the speed
(bytes/sec) decreases as K increases. We observe an inverse
relationship between error (JSD/byte) and speed (bytes/sec):
as the processing speed (bytes/sec) decreases, the error also
decreases. Notably, this tradeoff is non-linear, with error in-
creasing more sharply at higher processing speeds compared
to lower speeds. This trend is evident in all models; in gen-
eral, error appears to flatten out after K ≥ 8. This indicates
diminishing returns in reducing error as K gets large. We
hypothesize that this occurs because the language model’s
probability mass is concentrated on a limited set of tokeniza-
tions, which are adequately covered even with smaller beam
sizes. We also observe (as expected) that larger models run
more slowly; however, this appears to be due to their higher
evaluation time rather than the need for larger covers.

We also observe that the average per-byte surprisal is sig-
nificantly lower under all models than the canonically tok-
enized baseline. The reasons for this are twofold: (1) Each
model assigns non-negligible probability to noncanonical
tokenizations of the corpus, which are being thrown out
in the baseline estimate, but that is accounted for in our
estimate. (2) The most likely tokenization of the corpus
is often noncanonical, which our method is better able to
find, as our beam-summing method uses the probabilities
assigned to tokenizations. In contrast, the baseline uses
only the hard-coded canonical tokenization. Interestingly,
increasing K does not appear to significantly decrease the
surprisal, which we suspect is because the relatively greedy
(K = 2) tokenizations adequately cover it.18

Conclusion

We have developed an effective method for ameliorating
tensions between tokens and characters faced by engineers
and users. We gave theory and algorithms that provide a
character-level interface to tokenized language models. We
characterized and resolved the prompt boundary problem.
We investigated the empirical speed and error rates of our
method on two modern language models. The primary
limitation of our beam summing method is that it requires
a very large beam size K if the language model does not
favor a small number of tokenizations. The models that
we explored in our experiments concentrate mass on a few
tokenizations; thus, we did not require large K to estimate
their character-level prefix probabilities accurately.

Acknowledgments

The authors would like to thank Andreas Opedal, Alex Lew,
Ben Lipkin, Jacob Hoover Vigly, Luca Malagutti, Manuel de

18We found that K = 1 occasionally resulted in an empty beam,

so we do not include it in this experiment.

Prada Corral, Vésteinn Snæbjarnarson, Samuel Kiegeland,
and Yahya Emara for their helpful feedback and discussions.
JT would like to thank Rycolab for its hospitality during a
recent visit. The authors JLG and JT would like to thank
Institut des Hautes Études Scientifiques (IHES) for their
hospitality while revising this paper. MG was supported
by an ETH Zürich Postdoctoral Fellowship. This research
was enabled in part by compute resources provided by Mila
(mila.quebec).

Impact Statement

The primary impact of this work is a more predictable, user-
friendly interface for working with tokenized language mod-
els. By ensuring tools behave as expected, we mitigate
certain unintended behaviors, such as the prompt-boundary
problem highlighted in the introduction. More broadly, we
do not anticipate any additional risks beyond those already
inherent in the use of tokenized language models.

Limitations

Our method should work well with other tokenized language
models, provided they were trained with a deterministic tok-
enizer. However, if they were trained with a stochastic tok-
enizer, such as UnigramLM (Kudo, 2018) or BPE-Dropout
(Provilkov et al., 2020), we would expect that the probabil-
ity mass over tokenizations in each covering would not be
heavily concentrated on a small subset. Thus, these mod-
els may require a large beam size, making our approach
expensive. Sampling-based methods may provide a better
speed–accuracy tradeoff for this setting.

References

Cao, K. and Rimell, L. You should evaluate your language
model on marginal likelihood over tokenisations.
In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, 2021. URL https:
//aclanthology.org/2021.emnlp-main.161.

Chirkova, N., Kruszewski, G., Rozen, J., and Dymetman,
M. Should you marginalize over possible tokenizations?
In Proceedings of the Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers),
2023. URL https://aclanthology.org/2023.acl-
short.1.

Gage, P. A new algorithm for data compression. C
ISSN 0898-9788. URL

Users Journal, 12(2), 1994.
https://web.archive.org/web/20230319172720/
https://www.derczynski.com/papers/archive/
BPE_Gage.pdf.

Gastaldi, J. L., Terilla, J., Malagutti, L., DuSell, B., Vieira,

9

From Language Models over Tokens to Language Models over Characters

T., and Cotterell, R. The foundations of tokenization: Sta-
tistical and computational concerns. In Proceedings of the
International Conference on Learning Representations,
2025. URL https://arxiv.org/abs/2407.11606.

Geng, S., Josifoski, M., Peyrard, M., and West, R. Grammar-
constrained decoding for structured NLP tasks with-
In Proceedings of the Conference on
out finetuning.
Empirical Methods in Natural Language Processing,
2023. URL https://aclanthology.org/2023.emnlp-
main.674.pdf.

Giulianelli, M., Malagutti, L., Gastaldi, J. L., DuSell, B.,
Vieira, T., and Cotterell, R. On the proper treatment of
tokenization in psycholinguistics. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, 2024. URL https://aclanthology.org/
2024.emnlp-main.1032/.

Hale, J. A probabilistic Earley parser as a psycholinguistic
model. In Meeting of the North American Chapter of the
Association for Computational Linguistics, 2001. URL
https://aclanthology.org/N01-1021.pdf.

Koo, T., Liu, F., and He, L. Automata-based constraints
In Conference on Lan-
for language model decoding.
guage Modeling, 2024. URL https://openreview.
net/forum?id=BDBdblmyzY.

Kudo, T. Subword regularization: Improving neural network
translation models with multiple subword candidates. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers),
2018. URL https://aclanthology.org/P18-1007.

Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,
C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient
memory management for large language model serving
with PagedAttention. In Proceedings of the ACM SIGOPS
Symposium on Operating Systems Principles, 2023. URL
https://arxiv.org/abs/2309.06180.

Levy, R.
sion.
0277.
science/article/pii/S0010027707001436.

Expectation-based syntactic comprehen-
ISSN 0010-
URL https://www.sciencedirect.com/

Cognition, 106(3), 2008.

Loula, J., LeBrun, B., Du, L., Lipkin, B., Pasti, C., Grand,
G., Liu, T., Emara, Y., Freedman, M., Eisner, J., Cot-
terell, R., Mansinghka, V., Lew, A. K., Vieira, T., and
O’Donnell, T. J. Syntactic and semantic control of large
language models via sequential Monte Carlo. In Proceed-
ings of the International Conference on Learning Rep-
resentations, 2025. URL https://openreview.net/
forum?id=xoXn62FzD0.

10

Lundberg, S. and Ribeiro, M. T. The art of prompt
design: Prompt boundaries and token healing. Medium,
2023. URL https://towardsdatascience.com/the-
art-of-prompt-design-prompt-boundaries-and-
token-healing-3b2448b0be38.

Microsoft. Guidance. https://github.com/microsoft/

guidance, 2023.

Mikolov, T., Karafiát, M., Burget, L.,

ˇCernocký,
Recurrent neural network
J., and Khudanpur, S.
IN-
based language model.
TERSPEECH, 2010.
10.21437/Interspeech.
doi:
2010-343. URL https://www.isca-archive.org/
interspeech_2010/mikolov10_interspeech.html.

In Proceedings of

Oh, B.-D. and Schuler, W. Leading whitespaces of lan-
guage models’ subword vocabulary poses a confound
for calculating word probabilities, 2024. URL https:
//arxiv.org/abs/2406.10851.

Phan, B., Havasi, M., Muckley, M. J., and Ullrich, K.
Understanding and mitigating tokenization bias in lan-
guage models. In ICML Workshop on Theoretical Foun-
dations of Foundation Models, 2024. URL https:
//openreview.net/forum?id=OqfdrBj1y1.

Pimentel, T. and Meister, C. How to compute the prob-
In Proceedings of the Conference
ability of a word.
on Empirical Methods in Natural Language Processing,
2024. URL https://aclanthology.org/2024.emnlp-
main.1020/.

Poesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,
C., and Gulwani, S. Synchromesh: Reliable code genera-
tion from pre-trained language models. In Proceedings
of the International Conference on Learning Represen-
tations, 2022. URL https://openreview.net/forum?
id=KmtVD97J43e.

Provilkov, I., Emelianenko, D., and Voita, E. BPE-
Dropout: Simple and effective subword regularization.
In Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics, 2020. URL https:
//aclanthology.org/2020.acl-main.170/.

Radford, A., Wu, J., Child, R., Luan, D., Amodei,
D., Sutskever, I., et al. Language models are un-
supervised multitask learners. OpenAI blog, 1(8),
2019. URL https://d4mucfpksywv.cloudfront.net/
better-language-models/language-models.pdf.

Scholak, T., Schucher, N., and Bahdanau, D. PICARD:
parsing incrementally for constrained auto-regressive
In Proceedings
decoding from language models.
of the Conference on Empirical Methods in Natural,
2021. URL https://doi.org/10.18653/v1/2021.
emnlp-main.779.

From Language Models over Tokens to Language Models over Characters

Sennrich, R., Haddow, B., and Birch, A. Neural ma-
chine translation of rare words with subword units. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, 2016. URL https://
aclanthology.org/P16-1162.

Shannon, C. E. A mathematical theory of communica-
tion. The Bell System Technical Journal, 27(4), 1948.
URL https://doi.org/10.1002/j.1538-7305.1948.
tb00917.x.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-
tention is all you need. Advances in Neural Infor-
mation Processing Systems, 30, 2017. URL https:
//arxiv.org/abs/1706.03762.

Willard, B. T. and Louf, R. Efficient guided genera-
tion for large language models. CoRR, abs/2307.09702,
2023. URL https://doi.org/10.48550/arXiv.2307.
09702.

Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,
Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite,
Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M.,
Lhoest, Q., and Rush, A. Transformers: State-of-the-
art natural language processing. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing: System Demonstrations, 2020. URL https:
//aclanthology.org/2020.emnlp-demos.6.

11

From Language Models over Tokens to Language Models over Characters

A. Supplementary Results

This section presents the data for Fig. 1 in tabular form.

A.1. Jensen–Shannon Distance

Llama-3.2-1B

Meta-Llama-3.1-8B

JSD / byte

bytes / sec

JSD / byte

bytes / sec

0.00025 (0.00010, 0.00048)
0.00006 (0.00005, 0.00007)
0.00004 (0.00004, 0.00005)
0.00004 (0.00003, 0.00004)
0.00004 (0.00004, 0.00004)
0.00003 (0.00003, 0.00004)
(not applicable)

115.18 (110.54, 117.94)
91.22 (88.36, 92.97)
64.71 (63.50, 65.55)
38.77 (37.77, 39.62)
20.53 (19.93, 21.11)
9.77 (9.52, 10.02)
4.91 (4.81, 5.02)

0.00016 (0.00010, 0.00023)
0.00005 (0.00004, 0.00005)
0.00004 (0.00004, 0.00004)
0.00003 (0.00003, 0.00004)
0.00003 (0.00002, 0.00003)
0.00002 (0.00002, 0.00002)
(not applicable)

36.03 (35.90, 36.16)
32.06 (31.74, 32.27)
26.72 (26.43, 26.95)
19.03 (18.76, 19.28)
11.42 (11.21, 11.62)
5.91 (5.81, 6.02)
2.97 (2.93, 3.02)

DeepSeek-R1-Distill-Llama-8B

phi-4 (14B)

JSD / byte

bytes / sec

JSD / byte

bytes / sec

0.00022 (0.00015, 0.00032)
0.00009 (0.00007, 0.00014)
0.00005 (0.00004, 0.00006)
0.00004 (0.00003, 0.00005)
0.00005 (0.00003, 0.00007)
0.00002 (0.00002, 0.00002)
(not applicable)

36.64 (36.47, 36.83)
32.60 (32.24, 32.87)
26.81 (26.52, 27.04)
19.26 (18.99, 19.52)
11.62 (11.42, 11.83)
5.96 (5.86, 6.07)
3.01 (2.97, 3.06)

0.00065 (0.00034, 0.00116)
0.00012 (0.00008, 0.00017)
0.00004 (0.00003, 0.00006)
0.00008 (0.00004, 0.00012)
0.00003 (0.00003, 0.00004)
0.00002 (0.00002, 0.00002)
(not applicable)

21.17 (21.03, 21.30)
19.09 (18.98, 19.18)
16.79 (16.69, 16.88)
12.81 (12.68, 12.94)
7.90 (7.78, 8.01)
4.16 (4.10, 4.22)
2.44 (2.40, 2.48)

K

2
4
8
16
32
64
128

K

2
4
8
16
32
64
128

12

From Language Models over Tokens to Language Models over Characters

A.2. Surprisal

Model

Llama-3.2-1B
Meta-Llama-3.1-8B
DeepSeek-R1-Distill-Llama-8B
phi-4 (14B)

Canonically Tokenized Baseline
bits / byte

0.76 (0.69, 0.83)
0.66 (0.59, 0.74)
0.93 (0.83, 1.03)
0.68 (0.61, 0.76)

Llama-3.2-1B

Meta-Llama-3.1-8B

bits / byte

bytes / sec

bits / byte

bytes / sec

0.63539 (0.58988, 0.68280)
0.63593 (0.59076, 0.68238)
0.63540 (0.58932, 0.68020)
0.63518 (0.58985, 0.68186)
0.63524 (0.58988, 0.68187)
0.63525 (0.59041, 0.68259)
0.63550 (0.59101, 0.68157)

115.18 (110.54, 117.94)
91.22 (88.36, 92.97)
64.71 (63.50, 65.55)
38.77 (37.77, 39.62)
20.53 (19.93, 21.11)
9.77 (9.52, 10.02)
4.91 (4.81, 5.02)

0.53339 (0.49164, 0.57577)
0.53173 (0.49087, 0.57468)
0.53202 (0.49109, 0.57463)
0.53208 (0.49058, 0.57452)
0.53176 (0.49012, 0.57411)
0.53180 (0.48991, 0.57464)
0.53175 (0.49091, 0.57396)

36.03 (35.90, 36.16)
32.06 (31.74, 32.27)
26.72 (26.43, 26.95)
19.03 (18.76, 19.28)
11.42 (11.21, 11.62)
5.91 (5.81, 6.02)
2.97 (2.93, 3.02)

DeepSeek-R1-Distill-Llama-8B
bits / byte

bytes / sec

phi-4 (14B)

bits / byte

bytes / sec

0.76195 (0.70150, 0.82342)
0.76134 (0.70160, 0.82244)
0.76136 (0.70270, 0.82294)
0.76101 (0.70137, 0.82419)
0.76080 (0.70129, 0.82283)
0.76128 (0.70313, 0.82384)
0.76093 (0.70029, 0.82178)

36.64 (36.47, 36.83)
32.60 (32.24, 32.87)
26.81 (26.52, 27.04)
19.26 (18.99, 19.52)
11.62 (11.42, 11.83)
5.96 (5.86, 6.07)
3.01 (2.97, 3.06)

0.56296 (0.51968, 0.60938)
0.56345 (0.51961, 0.60944)
0.56194 (0.51601, 0.60854)
0.56228 (0.51854, 0.60802)
0.56192 (0.51807, 0.60742)
0.56206 (0.51752, 0.60760)
0.56185 (0.51753, 0.60815)

21.17 (21.03, 21.30)
19.09 (18.98, 19.18)
16.79 (16.69, 16.88)
12.81 (12.68, 12.94)
7.90 (7.78, 8.01)
4.16 (4.10, 4.22)
2.44 (2.40, 2.48)

K

2
4
8
16
32
64
128

K

2
4
8
16
32
64
128

13

From Language Models over Tokens to Language Models over Characters

B. Proof of Proposition 1

Proposition 1. Suppose (Σ, ∆, τ , κ) is a tokenization model where κ is strict-prefix monotone and p∆ is a token-level
language model. Then, the prefix probability −→pΣ(σ) for the character-level model Eq. (6) is given by

−→pΣ(σ) =

(cid:88)

−→p∆(δ),

∀σ ∈ Σ∗

δ∈C(σ)

Proof. We prove the proposition through the following manipulations.

−→pΣ(σ) =

(cid:88)

1{σ ⪯ κ(δ′)}p∆(δ′)

δ′∈∆∗

= 1{σ = ε}p∆(ε) +

(cid:88)

1{σ ⪯ κ(δ′)} p∆(δ′)

= 1{σ = ε}p∆(ε) +

1(cid:8)κ(δ) ≺ σ ⪯ κ(δ·δ′·(cid:26)(cid:26)δ′′)(cid:9) p∆(δ·δ′·δ′′)

δ′∈∆+
(cid:88)

= 1{σ = ε}p∆(ε) +

1{κ(δ) ≺ σ ⪯ κ(δ·δ′)}

δ·δ′·δ′′∈∆+
(cid:88)

(cid:88)

p∆(δ·δ′·δ′′)

= 1{σ = ε}p∆(ε) +

1{κ(δ) ≺ σ ⪯ κ(δ·δ′)} −→p∆(δ·δ′)

δ·δ′∈∆+
(cid:88)

δ′′∈∆∗

δ·δ′∈∆+

−→p∆(δ)

(cid:88)

=
δ∈C(σ)

(13)

(16)

(17)

(18)

(19)

(20)

(21)

About the steps above: We start with the summation expression for the character-level prefix probability (i.e., Eq. (11)).
We expand the summation into two cases (so that it will eventually match the two cases in the expression for the covering
Eq. (12)). Next, for each summand, we consider its unique minimal prefix δ·δ′ covering σ. We see why ε is handled
separately, as it cannot be covered by a token sequence of that form. We exploit the key property of prefix monotone
tokenizers (i.e., that once δ·δ′ covers σ, each extension δ·δ′δ′′ continues to cover it). This allows us to rearrange the
summation to sum over the extension δ′′, which is conveniently equal to the prefix probability of δ·δ′. The final step is to
■
recognize that the summands can all be indexed by the covering C(σ).

14

From Language Models over Tokens to Language Models over Characters

C. The Size of the Covering

We now set about to bound the worst-case size of the covering function. To do so, we introduce additional definitions that
characterize the different growth factors.

We define κ’s fertility as

F def= max
δ∈∆∗

max
σ∈Σ∗

|{δ′ ∈ ∆ : σ = κ(δ·δ′)}| ≤ |∆|

(22)

Example 3. The BPE tokenizer has FBPE = 1 because it is multiplicative, and its tokens represent distinct substrings.
More formally,

FBPE = max
δ∈∆∗
= max
δ∈∆∗

max
σ∈Σ∗
max
σ∈Σ∗

|{δ′ ∈ ∆ : σ = κ(δ·δ′)}|

|{δ′ ∈ ∆ : σ = κ(δ)·κ(δ′)}|

= |{δ′ ∈ ∆ : σ′ = κ(δ′)}|
= 1

[def of fertility]

[def multiplicativity]

[def function]

[distinctness]

Additionally, we define a κ’s munch as follows.

M def= max
δ∈∆∗

max
δ∈∆

|κ(δ·δ)| − |κ(δ)|

(23)

(24)

(25)

(26)

(27)

In words, the munch measures the length of the largest number of characters that can be introduced by adding one more
token to any given context.

Example 4. The munch of a multiplicative κ, such as BPE, is maxδ∈∆ |κ(δ)|. Put in words, it is the length of the longest
detokenization. The munch for GPT-2 is surprisingly long (128).

Proposition 3. Let F and M be the fertility and munch of κ. Then, for all σ ∈ Σ∗,

|C(σ)| ≤ C(|σ|)

where

C(n) def=






0
1

if n < 0
if n = 0

n−1
(cid:88)

F
j=n−M

C(j) otherwise)

(28)

(29)

Proof. The base cases N ≤ 0 are straightforward. Consider the case of a string of length N ≥ 0. Inductive hypothesis:
Suppose for all strings σ′ with |σ′| < N , |C(σ′)| ≤ C(|σ′|).

15

From Language Models over Tokens to Language Models over Characters

Let σ be an arbitrary string with length N > 0.

|C(σ1 ··· σN )|

= (cid:12)
(cid:8)δ·δ ∈ ∆+ : κ(δ) ≺ σ1 ··· σN ⪯ κ(δ·δ)(cid:9)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j=0
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:8)δ·δ ∈ ∆+ : κ(δ) = σ1 ··· σj, σ1 ··· σN ⪯ κ(δ·δ)(cid:9)
(cid:12)
(cid:12)
(cid:12)
(cid:124)
(cid:125)
(cid:123)(cid:122)
(cid:12)
=∅ if N −(j+1)>M or N =j+1

N
(cid:91)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
{δ ∈ ∆∗ : κ(δ) = σ1 ··· σj}
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:124)
(cid:125)
(cid:123)(cid:122)
(cid:12)
(cid:12)
⊆C(σ1···σj )

· |{δ ∈ ∆ : δ ∈ ∆∗, σ1 ··· σN ⪯ κ(δ·δ)}|
(cid:124)
(cid:123)(cid:122)
(cid:125)
≤F

N −1
(cid:88)

≤

j=N −M

N
(cid:88)

≤ F ·

j=N −M

|C(σ1 ··· σj)|
(cid:125)
(cid:123)(cid:122)
(cid:124)
inductive hypothesis

N
(cid:88)

C(j)

≤ F ·

j=N −M

= C(N )

Thus, the proposition holds true by the principle of induction.

Corollary 2. Let N = |σ|. Consider the following cases:

• When M = N and F = 1, C(N ) = 2N .
• When M = N and F ≥ 0, C(N ) = F (1 + F )N .
• Otherwise, C(N ) = F N Fib(N, M ) where Fib(N, M ) is the N th M th-order Fibonacci number.19

In all cases, C F

M (N ) < ∞.

In the proposition below, we show that the covering can easily be exponential in size:

Proposition 4.

|C(σ)| ∈ Ω(2|σ|)

Proof. We prove the proposition by constructing an example that achieves the lower bound.

• Let Σ = {a}, ∆ =

(cid:26)

a

(cid:27)
1, aa

2

.

(30)

(31)

(32)

(33)

(34)

(35)

(36)

■

(37)

• Let κ be multiplicative, and define κ(a
• Let σ be an arbitrary string from Σ∗. Let N = |σ|.

1) def= a, κ(aa

2 ) def= aa.

Then, |E(σ)| ≥ the number of nonnegative integer solutions (n, m) to 1 m + 2 n = N . This is because we can build the aN
aa
2 accounts for 2. So if n is the number of token 1 and m is
using a sequence of
the number of token 2, we must have that 1 m + 2 n = N . The number of solutions grows like Ω(2N ). Lastly, because
■
C(σ) ⊇ E(σ), we have that |C(σ)| ∈ Ω(2|σ|). Thus, the proposition holds.

a
1 accounts for 1 a and each

aa
2 , but each

a
1 or

19M th-order Fibonacci numbers are a variation of the well-known Fibonacci (i.e., M = 2) that sums the previous M numbers in the

sequence instead of the previous two.

16

From Language Models over Tokens to Language Models over Characters

D. Bundled Beam Summing Implementation

Upon implementing this scheme, we observed that it is possible to efficiently reason about all the next tokens that extend a
given token sequence in the cover in bulk. The key idea is to group the token sequences that fall into the same bucket in
prune_top_K_buckets into a Bundle (see below) that represents them compactly. In particular, we can use a probability
trie to efficiently filter out the next tokens that disagree with the next character. This improves the per-iteration cost of that
filter as the data structure does the organizational work ahead of time (in bulk). We can regard the probability trie as a local
language model that generates the next token character-by-character according to the probability assigned to it by −→p∆(· | δ).
Each bundle can be unbundled (see method unbundle) into the respective tuples that the enum_cover algorithm maintains.
The algorithms are otherwise equivalent.

54 def beam(σ1 ··· σN ):
55

if N = 0: return [Bundle(1, ε, ε, build_trie(ε))]
candidates ← []
for bundle in beam(σ1 ··· σN −1):

filtered_bundle ← bundle.filter(σN )
if filtered_bundle is not None:

candidates.append(filtered_bundle)
for extended_bundle in bundle.extend():

candidates.append(extended_bundle.filter(σN ))

# Keep top-K bundles according to their prefix probability
return topK(candidates, key=lambda bundle: -bundle.p)

56

57

58

59

60

61

62

63

64

Each bundle is an instance of the following class with four member variables: the prefix probability p, token string δ,
character string σ, and a reference to a local probability trie trie. The trie provides the character-level probabilities of the
distribution over possible next tokens: p∆(· | δ). The trie is also augmented with a special symbol EOT to denote the end of
this next token.20

65 class Bundle(p, δ, σ, trie):

66

67

68

69

70

71

72

73

74

75

def filter(σ′):

if trie.p(σ′ | σ) = 0: return
return Bundle(p · trie(σ′ | σ), δ, σ·σ′, trie)

# no tokens give prefix σ·σ′ prob

def extend():

Z ← trie.p(EOT | σ)
if Z > 0: # emit tokens that decode to σ

for (δ′, p′) in trie.tokens[σ]:

yield Bundle( p

Z · p′, δ·δ′, ε, build_trie(δ·δ′))

The code below builds the probability trie from the possible next tokens. It figures out the character strings associated with
those next tokens and puts them into the trie with the respective probabilities.21

76 def build_trie(δ):

77

78

79

80

81

82

σ ← κ(δ)
trie ← ProbabilityTrie()
for δ′ ∈ ∆:

σ·σ′ ← κ(δ·δ′)
trie.add (σ′, δ′, p∆(δ′ | δ))

return trie

# remember common prefix
# uses EOT to mark the end of a token

# use new characters, i.e., ignore prefix σ
# add σ′ and δ′ to trie with this probability

The probability trie has the following functionality:

• trie.tokens[σ′] stores the set of tokens that decode to σ′ along with their respective mass.

20This is completely analogous to how EOS marks the end of a string.
21Note that EOS is handled as an indivisible symbol in the probability trie, whereas other tokens can be expanded into characters (or bytes).

17

From Language Models over Tokens to Language Models over Characters

• trie.p(σ′ | σ) returns the probability of the character σ′ given σ, it is proportional to

trie.p(σ′ | σ) ∝

p∆(δ′ | δ)

(cid:88)

δ′∈∆

(cid:40)1{κ(δ·δ′) = σ}
1{κ(δ·δ′) ⪰ σ·σ′}

if σ′ = EOT
otherwise

(38)

We note that the trie implicitly depends on the token string δ used in its creation.

To aid in understanding how the bundled algorithm relates to the original algorithm, we give the following methods.

83 class Bundle(p, δ, σ, trie):

84

85

86

87

88

89

90

91

92

93

94

95

...
def unbundle():
agenda ← [σ]
while agenda:

σ′ ← agenda.pop()
Z ← trie.p(EOT | σ′)
if Z > 0:

for (δ′, p′) ∈ trie.tokens[σ′]:

yield (p · p′, κ(δ·δ′), δ·δ′)

for σ′′ in trie.p(· | σ′):

if σ′′ ̸= EOT:

agenda.append(σ′·σ′′)

96 def unbundle_beam(beam):

97

return [item for bundle in beam for item in bundle.unbundle()]

We note that unbundle_beam(beam(σ)) gives precisely the same set of elements as the unbundled algorithm (i.e.,
enum_cover(σ)) run on the same string σ and the bucket-based pruning scheme with parameter K (up to reordering).

To compute the next-character probability, we use the following algorithm:

98 def next_character_probability(σ, method=beam):

99

100

101

102

103

104

105

106

107

p ← {σ′: 0 for σ′ ∈ Σ ∪ {EOS}}
for bundle in method(σ): # use beam approximation by default

for σ′ ∈ Σ:

p(σ′) += bundle.p · bundle.trie.p(σ′ | bundle.σ)

for ext_bundle in bundle.extend():

for σ′ ∈ Σ:

p(σ′) += ext_bundle.p · ext_bundle.trie.p(σ′ | ext_bundle.σ)

Z ← sum(p.values())
return {σ′: p(σ′)/Z for σ′ ∈ Σ}

18

From Language Models over Tokens to Language Models over Characters

E. Proof of Proposition 2

Proposition 2. conditional_token_generation(σ) generates samples according to p∆|Σ(· | σ) for all σ ∈ Σ∗.

Proof (Sketch). Choose an arbitrary δ ∈ ∆∗.

p∆|Σ(δ | σ) = P

Y ∼p∆

[Y = δ | κ(Y ) ⪰ σ]

= p∆(δ)

= p∆(δ)

[κ(Y ) ⪰ σ]

1{κ(δ) ⪰ σ}
P
Y ∼p∆
1{κ(δ) ⪰ σ}
−→pΣ(σ)

Let δ′ = ϕσ(δ) (i.e., the shortest prefix of δ such that κ(δ′) ⪰ σ). Choose δ′′ such that δ′·δ′′ = δ.

= −→p∆(EOS | δ′·δ′′)−→p∆(δ′′ | δ′)
(cid:123)(cid:122)
(cid:125)
sample completion

(cid:124)

−→p∆(δ′)
(cid:124)

1{κ(δ′) ⪰ σ}
−→pΣ(σ)
(cid:123)(cid:122)
sample from covering

(cid:125)

(39)

(40)

(41)

(42)

We can see that the algorithm samples from this distribution because it samples a token string δ′ from the covering in
proportion to the right factor in Eq. (42) and then samples a completion δ′′ of δ′ in proportion to the left factor of the
equation. Thus, the sample δ = δ′·δ′′ has probability p∆|Σ(δ | σ), and conditional_token_generation(σ) is a correct
■
sampling procedure for it.

19

From Language Models over Tokens to Language Models over Characters

F. Token Healing Baseline

We implemented an additional baseline character-level model based on a token healing heuristic described in §1. While not
competitive with our proposed methods, we include it here for completeness. Explain the idea, and provide pseudocode
below. The experimental results are summarized in Tab. 1. We observe significantly higher surprisal (bits/byte) and error
(JSD/bytes) compared to the methods in Fig. 1.

An approximate covering inspired by token healing. Our version of the token healing heuristic is based on an
approximate covering. Given character string σ ∈ Σ∗ and its tokenization δ1 ··· δN = τ (σ),22 we define its token healing
approximate covering as follows:

Cheal(σ) def= {δ1 ··· δN −1·δ′ | δ′ ∈ ∆, κ(δN ) ⪯ κ(δ′)}

(43)

By construction, κ(δ1 ··· δN −1) ≺ σ ⪯ κ(δ1 ··· δN ), so it is straightforward to see that Cheal(σ) ⊆ C(σ). We also note that
the size of this approximate covering is bounded by the size of the token alphabet: |Cheal(σ)| ≤ |∆|.

Implementation. The code below implements a method for efficiently constructing Cheal(σ). It is based on the bundled
implementation given in App. D.

108 def token_healing_cover(σ):

109

110

111

112

113

114

115

if σ = ε: # no tokens to heal

return [Bundle(1, ε, ε, build_trie(ε)]

else

δ1 ··· δN ← τ (σ)
trie ← build_trie(δ1 ··· δN −1)
σ′ ← κ(δN )
return [Bundle(−→p∆(δ1 ··· δN −1) · trie.p(σ′ | ε), δ1 ··· δN −1, σ′, trie)]

We first tokenize the input character sequence σ into a sequence of tokens δ. We then construct a probability trie for
δ1 ··· δN −1 (i.e., all but the last token), allowing us to compute the probability of each possible next character σ′ by querying
the trie with the character suffix σ′ corresponding to the final token.

Compared to the beam algorithm, token_healing_cover method represents an extreme form of pruning: it returns a beam
of exactly one bundle. It also differs from the variant of token healing we described in §1, which produces token-level
predictions rather than character-level predictions.
To estimate −→pΣ(· | σ) we use next_character_probability(σ, token_healing_cover). However, this approximation
still fails to place an appropriately high probability on d when conditioned on Hello,␣worl). This happens for the same
reasons as we described in §1.

Experimental results. The table below presents experimental results that may be compared to those of other methods in
the paper. We found that the method performs poorly enough that we do not consider it a competitive approximation.

Model

Surprisal (bits/byte)

Error (JSD/byte)

Speed (bytes/sec)

DeepSeek-R1-Distill-Llama-8B 2.529 (2.391, 2.670)
2.126 (2.010, 2.246)
Llama-3.2-1B
2.116 (1.992, 2.243)
Meta-Llama-3.1-8B
2.204 (2.075, 2.333)
phi-4 (14B)

0.1907 (0.1800, 0.2015)
0.1790 (0.1691, 0.1891)
0.1840 (0.1738, 0.1946)
0.1857 (0.1757, 0.1960)

36.70 (36.64, 36.76)
119.35 (118.73, 119.84)
36.61 (36.55, 36.67)
21.24 (21.13, 21.30)

Table 1: Surprisal, JSD, and speed for the token-healing baseline across models using the same experimental settings as Fig. 1.

22If σ = ε, Cheal(ε) def= ∆.

20

From Language Models over Tokens to Language Models over Characters

G. Probability-Based Pruning Heuristic

This section presents some additional experimental results with a more extensive pruning method, based on the relative
contribution of a token string to the current cover approximation.

Probability-based pruning heuristic. The method works by augmenting the beam pseudocode with some additional
pruning based on a threshold θ ∈ [0, 1]. The benefit of this pruning is that it can avoid calls to bundle.extend(), which
requires evaluating the language model (i.e., the performance bottleneck of our algorithm). It is often the case that for a
given character string prefix, there is no reason to perform any extend operations because all tokenizations of that string are
low probability; thus, filtering operations are needed.

118

119

120

121

122

123

124

125

126

127

128

129

130

116 def beamK,θ(σ1 ··· σN ):
117

if N = 0: return [Bundle(1, ε, ε, build_trie(ε))]
candidates ← []
B ← beam(σ1 ··· σN −1)
for bundle in B:

filtered_bundle ← bundle.filter(σN )
if filtered_bundle is not None:

candidates.append(filtered_bundle)

τ ← θ · sum(bundle.p for bundle in candidates)
for bundle in B:

if bundle.p ≥ τ :

for extended_bundle in bundle.extend():

candidates.append(extended_bundle.filter(σN ))
# Keep top-K bundles according to their prefix probability
return topK(candidates, key=lambda bundle: -bundle.p)

21

From Language Models over Tokens to Language Models over Characters

Experiments. Below, we extend the experiments of the main text with this additional pruning heuristic to better understand
its speed and accuracy.

(a) Error (JSD/byte) vs. speed (bytes/sec). Here we compute the JSD between the byte-level conditional distributions computed with
beamK,θ for (K, θ) ∈ {4, 8, 16, 32, 64} × {0, 0.001, 0.0025, 0.01} and the reference model (K = 128, θ = 0). Missing values
indicate “dead-ending” (i.e., empty beam) as a result of overly aggressive pruning.

(b) Average surprisal (bits/byte) vs. speed (bytes/sec) for beamK,θ with {4, 8, 16, 32, 64} × {0, 0.001, 0.0025, 0.01}. The dotted line
represents surprisal of the canonically tokenized under the token-level language model. Missing values indicate infinite surprisal as a
result of excessive pruning.

Figure 2: Additional experimental results showing the effects of the pruning thresholds on error vs. speed and surprisal vs.
speed using the same settings as in §4.

22


